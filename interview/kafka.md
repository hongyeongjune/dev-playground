### Broker 란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

카프카 브로커는 카프카 클라이언트와 데이터를 주고받기 위해 사용하는 주체이자, 데이터를 분산 저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션  
보통 3대 이상의 브로커 서버를 1개의 클러스터로 묶어서 운영  
프로듀서로부터 전달받은 데이터는 파일 시스템에 저장  
페이지 캐시(OS에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역)를 사용하여 디스크 입출력 속도를 높여 속도 이슈를 해결 => 힙 메모리 사이즈를 크게 조절할 필요가 없음.

브로커 종류
* Controller Broker : 주로 클러스터 정상 동작을 보장
  * 파티션 리더 선출 : Kafka에서는 각 파티션에 대해 여러 개의 복제본(replica)이 존재하며, 그중 하나가 리더로 선출됩니다. 컨트롤러는 파티션의 리더를 관리하며, 리더가 다운되었을 때 새로운 리더를 선출하는 역할을 수행합니다.
  * 브로커 장애 처리 : 클러스터 내에서 브로커가 실패하거나 다시 복구되는 경우, 컨트롤러는 그에 따라 파티션 리더와 복제본을 재조정합니다.
* Coordinator Broker : 주로 컨슈머 그룹 관련 작업
  * 오프셋 관리 : 컨슈머가 특정 파티션에서 메시지를 읽고 난 후, 어디까지 읽었는지(오프셋)를 기록하고 관리합니다. 이는 각 컨슈머가 자신의 오프셋을 지속적으로 업데이트하여, 재시작 시 이전에 읽었던 메시지부터 다시 처리할 수 있도록 합니다.
  * 파티션 할당: 컨슈머 그룹 내에서 각 컨슈머에게 파티션을 할당하는 작업을 수행합니다. 이 작업을 통해 여러 컨슈머가 같은 파티션을 동시에 처리하지 않도록 조정합니다.

</details>

### Topic 이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

카프카는 토픽(Topic)이라는 곳에 데이터를 저장한다.  
토픽은 병렬 처리를 위해 여러개의 파티션(Partition)이라는 단위로 나뉜다.  
파티션에 메세지가 저장되어 있는 위치를 오프셋(Offset)이라고 부르며, 오프셋은 순차적으로 증가하는 숫자 형태로 되어있다.  
하나의 파티션 안에서의 오프셋은 고유한 숫자로, 카프카에서는 오프셋을 통해 메세지의 순서를 보장하고 컨슈머에서는 마지막까지 읽은 위치를 알 수 있다.  

![image](https://github.com/user-attachments/assets/57285c9b-1fba-47ca-970f-e7c82d1c2dff)

</details>


### Replication 이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

각 메세지들을 여러 개로 복제해서 카프카 클러스터내 브로커들에게 분산시키는 동작을 의미  
리플리케이션 동작 덕분에 하나의 브로커가 종료되더라도 카프카는 안정성을 유지할 수 있다.  
토핑 생성 명령어 중 replication-factor 라는 옵션으로 설정할 수 있다.  
예를 들어, 특정 토픽의 replication-factor 를 3으로 설정했다면, 해당 토픽의 파티션이 브로커로 리플리케이션이 된다.  
리플리케이션 팩터 수가 커지면 안정성은 높아지지만, 그만큼 브로커 리소스를 많이 사용하게 된다.  

카프카는 데이터 복제를 통해 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 동작하기 위해 파티션 단위로 복제된다.  
토픽을 생성할 때 파티션의 복제 개수도 같이 설정하는데 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션 값을 따라간다.  
복제 개수의 최솟값은 1(복제없음)이고 최댓값은 브로커 개수만큼 사용할 수 있다.  
예를 들어 복제 개수가 3(자신+복제2개)으로 총 3개의 파티션이 구성된다면 리더 파티션과 팔로워 파티션으로 구성된다.  
프로듀서 또는 컨슈머와 직접 통신하는 파티션을 리더 파티션, 복제 데이터를 갖는 나머지 파티션을 팔로워 파티션이라고 한다.  
팔로워 파티션들은 리더 파티션의 오프셋을 확인하여 현재 자신이 가지고 있는 오프셋과 차이가 나는 경우 리더 파티션으로부터 데이터를 가져와서 자신의 파티션에 복제한다.  
만약 리더 파티션을 갖고 있는 브로커에 장애가 발생해 다운되면 팔로워 파티션 중 하나가 리더 파티션 지위를 넘겨받는다.  
이를 통해 데이터가 유실되지 않고 컨슈머, 프로듀서가 데이터를 주고받도록 동작할 수 있게 된다.  

추천 수
* 테스트나 개발 환경 : 리플리케이션 팩터 수 1로 설정
* 운영 환경 (유실 허용하지 않음) : 리플리케이션 팩터 수 3으로 설정

</details>

### Partition 이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

하나의 토픽이 한 번에 처리할 수 있는 한계를 높이기 위해 토픽 하나를 여러개로 나눠 병렬 처리가 가능하게 만든 것을 파티션이라고 한다.  
파티션을 여러개로 나누면 병렬 처리가 가능해진다.  
파티션 수만큼 컨슈머를 연결할 수도 있다.  
파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 반대로 한 번 늘린 파티션은 절대로 줄일 수 없다.  
* 여러 이유가 있겠지만, 대표적인 이유로 다수 브로커에 분배되어 있는 세그먼트를 다시 재배열하는 것에 상당한 리소스가 들어가기 때문이다.  
* KIP-694(https://cwiki.apache.org/confluence/display/KAFKA/KIP-694%3A+Support+Reducing+Partitions+for+Topics)에서 파티션 개수를 줄이는 방안에 대해 논의했으나 더 이상 진행되고 있지는 않다.  
* 따라서, 초기에 생성할 때 파티션 수를 작게 설정하고 메세지 처리량이나 컨슈머 LAG 을 모니터링하면서 조금씩 늘려가는게 좋다.  

</details>

### 파티셔너란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

프로듀서는 토픽으로 메세지를 보낼 때 해당 토픽의 어느 파티션으로 메세지를 보내야 할지를 결정해야한다.  
이때, 파티셔너는 데이터를 어떤 파티션에 넣을지 결정하는 역할을 한다.  
프로듀서가 파티션을 결정하는 알고리즘은 기본적으로 메세지의 키를 해싱하여 파티션을 결정한다.  
따라서, 메세지의 키 값이 동일하다면 해당 메세지들은 모두 동일한 파티션에 전송된다.  
다만, 파티션 수가 변경됨과 동시에 메세지의 키와 매핑된 해시 테이블이 변경될 수 있기 때문에 파티션의 수를 늘리면 다른 파티션으로 전송될 수 있다.  

</details>

### Segment 란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

요약 : 카프카 프로듀서에 브로커로 전송된 메세지는 토픽의 파티션에 저장되며, 각 메세지들은 세그먼트라는 로그 파일의 형태로 브로커의 로컬 디스크에 저장된다.

![image](https://github.com/user-attachments/assets/9cd5e806-80fd-4614-9563-b27413cb1fe5)

카프카의 토픽으로 들어오는 메세지(Record)는 로그 세그먼트(Log Segment, 세그먼트라고도함) 파일에 저장된다.  
세그먼트에는 메시지의 내용, 키, 벨류, 오프셋, 메세지 크기 같은 정보가 저장된다.  
세그먼트 파일은 브로커의 로컬 디스크에 보관된다.  

세그먼트 크기가 너무 커져버리면 파일을 관리하기 어렵기 때문에, 세그먼트의 최대 크기는 1GB 가 기본값으로 설정되어있다.  
세그먼트 파일은 로그 세그먼트 인덱스 파일을 가지고 있다. 이 파일은 세그먼트 파일에 있는 메세지의 오프셋과 위치를 가지고 있다.  
세그먼트 크기가 1GB 보다 커지면 기본적으로 롤링 전략을 사용한다.  
다시 말해, 1GB 가 넘어가면 해당 파일을 클로즈하고, 새로운 세그먼트 파일을 만든다.  
카프카 관리자는 1GB 크기의 세그먼트 파일이 무한정 늘어나는 경우를 고려해 관리 계획을 수립해야한다.  
세그먼트 삭제 정책은 대표적으로 삭제, 압축이 있다.  

세그먼트 삭제  
* 세그먼트 삭제 정책은 디폴트이기 때문에 따로 명시하지 않아도 된다.
* 명시하게되면 다음과 같은 옵션을 사용한다. -> log.cleanup.policy : delete
* retention.ms 옵션을 사용하면 세그먼트 보관 시간이 해당 숫자보다 크면 삭제한다는 명령이다. -> 기본값 7일
* 세그먼트의 삭제 작업은 일정 주기를 가지고 체크하는데, 카프카의 기본값은 5분 주기로 되어있다. 따라서 5분 주기마다 삭제된다.
* retention.bytes 값을 사용하여 지정된 크기를 기준으로도 삭제할 수 있다.

세그먼트 압축  
* 삭제하지 않고, 압축하여 보관할 수 있다.
* 압축할지라도 카프카의 로컬 디스크에 무한정 보관하면, 로그의 용량은 감당할 수 없이 커져서 디스크가 가득차게 될 수 있다.
* 따라서, 카프카는 단순하게 압축하는 것이 아닌, 메세지의 키 값을 기준으로 마지막 데이터만 보관한다.
* 컨슈머 그룹은 항상 마지막으로 커밋된 오프셋 정보가 중요하므로, 과거에 커밋된 정보들은 삭제해도 괜찮다.
* 로그 컴팩션의 장점은 빠른 장애 복구를 할 수 있다.
* 장애 복구 시 전체 로그를 복구하지 않고, 메세지의 키를 기준으로 최신의 상태만 복구를 할 수 있다.
* 모든 토칙에 로그 컴팩션을 적용하는 것은 좋지 않다. 키값을 기준으로 최종값만 필요한 워크로드에 적용하는 것이 바람직하다.


</details>

### Offset 이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

레코드는 저장될 때 오프셋 값이 부여된다.  
오프셋은 컨슈머 그룹이 데이터를 어디까지 읽어갔는지 확인하는 용도로 사용된다.  
컨슈머 그룹은 토픽의 특정 파티션으로부터 데이터를 가져가서 처리하고 파티션의 어느 레코드까지 읽었는지 알리기 위해 데이터를 처리한 뒤 오프셋을 커밋한다.  
레코드가 브로커에 저장되면 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량 정책에 의해서만 삭제가 가능하다.  
로그 세그먼트 파일은 기본적으로 1GB의 크기를 채우면 닫히고 retention 옵션에 의해 삭제된다  

</details>

### Page Cache 란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

순차 I/O  
* 일반적으로 디스크 I/O 는 메모리에 비해 굉장히 느리다고 알려져 있지만, 그것은 Random Access 일 때 느리다.
* 순차 쓰기 성능은 약 600MBps
* 랜덤 쓰기 성능은 약 100Kbps로, 순차 쓰기에 비해 6,000배 이상 느리다.
* 특히 Random Access 의 경우 Disk 보다 RAM 이 느리다는건 모두가 아는 사실이다.
* Sequential Access 의 경우에는 Disk 와 RAM 간 큰 차이가 없다.
* 랜덤 액세스는 데이터를 검색하거나 저장하는 방법으로, 데이터에 어떤 순서로든 액세스할 수 있습니다.
* 순차 액세스는 데이터가 순차적인 순서로 액세스되는 데이터를 검색하거나 저장하는 방법입니다.
* 카프카는 쓰기(생산자가 데이터를 쓰는 것)와 읽기(소비자가 데이터를 소비하는 것)가 순차적으로 이루어지도록 설계되었습니다.
* Write
  * 카프카에서는 토픽을 사용하여 메시지를 그룹화합니다. 
  * 각 토픽은 여러 파티션으로 나뉩니다. 
  * 토픽의 각 파티션은 논리적 로그에 해당합니다. 
  * 물리적으로 로그는 거의 동일한 크기(예: 1GB)의 세그먼트 파일 집합입니다. 
  * 프로듀서가 파티션에 메시지를 게시할 때마다 브로커는 마지막 세그먼트 파일에 메시지를 추가합니다.
* Read
  * 컨슈머는 항상 특정 파티션의 메시지를 순차적으로 소비합니다.

* 이 시퀀셜 엑세스가 항상 보장되는 것은 아니다. 카프카의 데이터가 위치하는 파일 시스템을 다른 어플리케이션이 함께 사용하는 경우, 다른 어플리케이션으로 인해 디스크 단편화가 발생할 소지는 존재한다. 
* 카프카의 데이터를 가급적 독립된 파일시스템에 유지할 것을 권장하는 이유가 여기에 있다. 
* 하나의 파일 시스템을 카프카가 온전히 홀로 사용하도록 하여 카프카의 순차 I/O 를 보장해야 한다.
* ![images](https://github.com/user-attachments/assets/4b62b55c-ea3e-4d4e-8916-69ca1b30da3e)

페이지 캐시  
* 카프카는 페이지 캐시를 이용한다.
* 카프카는 JVM 위에서 동작한다.
* 
* 페이지 캐시는 운영체제가 디스크 I/O 성능을 높이기 위해, 파일의 내용을 메모리(RAM)에 저장해두는 캐시 영역이다.
* 읽기 시
  * 디스크에 읽기 요청 → Page Cache에 있으면 즉시 응답 (cache hit)
  * 디스크에 없으면 디스크에서 읽어오고 → Page Cache에 저장 → 다음 요청에 재사용 가능
* 쓰기 시
  * 디스크에 쓰기 요청 → Page Cache에 먼저 기록 (dirty page로 표시)
  * 일정 시점에 실제 디스크에 flush (fsync, sync, 또는 OS 정책)

![images](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JBIcXFxrdMtTcGB0.png)

Zero Copy
* Zero Copy(제로 카피)는 데이터를 복사하지 않고 처리하는 방식을 의미하며, 주로 파일 시스템이나 네트워크 I/O 성능 최적화에 사용됩니다.
* 기존 데이터 흐름
  * 디스크에서 파일을 읽고 네트워크를 통해 전송하는 일반적인 흐름에서 데이터는 일반적으로 사용자 모드와 커널 모드 간에 네 번의 컨텍스트 전환을 통해 네 번 복사됩니다. 
  * 이 흐름은 다음과 같은 단계로 구성됩니다
    1. 디스크의 파일 콘텐츠를 읽고 OS 페이지 캐시(읽기 버퍼)에 저장합니다. 이 단계에서는 사용자 모드에서 커널 모드로 컨텍스트 전환이 필요합니다.
    2. 데이터가 읽기 버퍼에서 애플리케이션 버퍼로 복사되므로 컨텍스트가 커널 모드에서 사용자 모드로 전환되어야 합니다.
    3. 그런 다음 데이터가 소켓 버퍼로 복사됩니다. 다시 한 번 컨텍스트를 사용자 모드에서 커널 모드로 전환해야 합니다.
    4. 소켓 버퍼로 데이터를 전송한 후 컨텍스트가 다시 사용자 모드로 전환됩니다. 그런 다음 소켓 버퍼에서 네트워크 인터페이스 컨트롤러(NIC)로 데이터를 복사합니다.
    5. NIC가 데이터를 대상으로 전송합니다.
  * ![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ho0t6F16eYkY0dPl.png)
* Zero Copy 데이터 흐름
  * 제로 카피 최적화를 사용하면 데이터가 페이지 캐시에서 소켓 버퍼로 직접 복사됩니다. 
  * 유닉스 기반에서 이 기술은 sendfile() 시스템 호출로 처리됩니다. 
  * 이 기술은 read() 및 write() 시스템 호출을 사용할 때 사용자 공간으로 데이터를 전송하지 않고 한 파일 설명자에서 다른 파일 설명자로 직접 데이터를 복사합니다. 
  * 따라서 이 최적화를 통해 Kafka는 원래의 데이터 전송 흐름에서 2단계와 3단계를 우회할 수 있습니다. 
  * Kafka가 제로 카피 기술을 활용하는 경우, 그 흐름은 다음과 같이 요약할 수 있습니다:
    1. 데이터가 디스크에서 페이지 캐시로 복사됩니다. 
    2. 그런 다음 sendfile() 호출을 통해 페이지 캐시에서 NIC(Network Interface Controller)로 데이터가 직접 복사됩니다. 
    3. NIC는 데이터를 대상으로 전송합니다. 
  * 결과적으로 컨텍스트 전환이 4개에서 2개로 줄어들고 데이터 복사본이 Kafka 애플리케이션으로 복사할 필요가 없습니다. 
  * 또한 1단계에서는 데이터를 읽을 때마다 메모리로 이동하여 사용자 공간으로 복사하는 대신 페이지 캐시에 정확히 한 번만 복사하여 필요할 때 재사용합니다.
  * ![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HxW0LkeGrKEvV3ID.png)

</details>

### 배치 전송과 압축 전송

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

배치 전송  
* 메시지를 쓰거나 읽을 때 개별 건마다 처리하지 않고 여러 건을 묶어서 일괄(batch) 처리하는 단순한 최적화를 통해 막대한 성능 개선 효과를 볼 수 있다.
* 일괄 처리는 한 번 전송에 사용되는 네트워크 패킷의 크기를 확대하므로 네트워크 I/O 오버헤드를 줄이고, 대규모의 순차 디스크 작업으로 이어지며, 연속적인 메모리 블록 사용으로 이어지는 등 효율 개선 효과가 매우 크다.

압축 전송  
* 카프카는 메세지 전송 시 좀 더 성능이 높은 압축 전송을 사용하는 것을 권장한다.
* 카프카에서 지원하는 압축 타입은 gzip, snappy, lz4, zstd 등이 있다.
* 압축만으로도 네트워크 대역폭이나 회선 비용 등을 줄일 수 있는데, 위의 배치 전송 처리와 결합해 파일 하나를 압축하는 것 보단 여러개를 압축해 전송하는 것이 더 효율이 좋기 때문이다.

</details>

### 프로듀서의 기본 동작은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

각 Record 들은 프로듀서의 send() 메서드를 통해 전달된다.  
각 Record 에 파티션을 지정했다면 지정된 파티션으로 가고 지정하지 않았다면 키를 가지고 파티션을 선택해 Record 를 전달하는데, 기본적으로 라운도로빈 방식으로 동작한다.  
프로듀서 내부에서는 send() 동작 이후 Record 들을 잠시 파티션별로 모아둔다. (배치 전송을 하기 위해서)  
전송이 실패하면 재시도 지정 횟수만큼 재시도가 이뤄지고 최종적으로 실패하면 최종 실패를 전달하며 성공하면 메타데이터를 리턴한다.  
프로듀서는 데이터를 전송할 때, 파티셔너에 의해 전송되는 파티션을 정한다.   
파티셔너에 의해 구분된 레코드는 전송되기 전에 Accumulator에 의해 데이터가 버퍼로 쌓이고 버퍼로 쌓인 데이터는 배치라는 묶음으로 한번에 전송된다.  

![image](https://github.com/user-attachments/assets/88c82676-b269-4469-975b-03313e2806ab)

</details>

### 컨슈머의 기본 동작은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

프로듀서가 카프카의 토픽으로 메세지를 전송하면, 해당 메세지들은 브로커들의 로컬 디스크에 저장된다.  
컨슈머는 토픽에 저장된 메세지를 가져올 수 있다.  
컨슈머 그룹은 하나 이상의 컨슈머들이 모여 있는 그룹을 의미하고, 컨슈머는 반드시 컨슈머 그룹에 속하게 된다.  
컨슈머 그룹은 각 파티션의 리더에게 카프카 토픽에 저장된 메세지를 가져오기 위한 요청을 보낸다.  
이 떄 파티션의 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적이다. (컨슈머 수가 파티션 수보다 많다고 해서 더 빠르게 토픽의 메세지를 가져오거나 처리량이 높아지는 것이 아닌 더 많은 수의 컨슈머들이 대기상태로 있기 때문)  
Active/Standby 개념으로 추가 컨슈머가 필요하다고 느낄 수 있지만, 컨슈머 그룹은 리밸런싱 동작을 통해 장애가 발생한 컨슈머의 역할을 동일한 그룹에 있는 다른 컨슈머가 그 역할을 대신 수행하므로 굳이 추가 리소스를 할당할 필요가 없다.  

Spring Kafak에서 오토 커밋으로 설정하면 오프셋을 주기적으로 커밋하게 된다. 다만, 컨슈머 종료가 빈번하게 일어나면 일부 메세지를 못 가져오거나 중복으로 가져오는 경우가 생길 수 있다.  
* auto.commit.enable = true
* auto.commit.interval.ms = 5000 (주기)

Spring Kafka에서 수동 커밋을 하기 위해서는 AckMode 를 설정하면 된다.  
* RECORD
  * 리스너가 레코드를 처리한 후 반환할 때 오프셋을 커밋한다.
* BATCH
  * poll()에서 반환된 모든 레코드가 처리되면 오프셋을 커밋한다.
  * 스프링 카프카 컨슈머의 AckMode 기본값
* TIME
  * poll()이 반환한 모든 레코드가 처리되면 마지막 커밋 이후 ackTime 이 초과되지 않는 한 오프셋을 커밋한다.
  * AckTime 옵션을 설정해야한다.
* COUNT
  * poll()에서 반환된 모든 레코드가 처리되면, 마지막 커밋 이후 ackCount 레코드가 수신된 경우에만 오프셋을 커밋한다.
  * AckCount 옵션을 설정해야한다.
* COUNT_TIME
  * TIME 및 COUNT 와 유사하지만 두 조건 중 하나라도 참이면 커밋이 수행된다.
* MANUAL
  * 메시지 수신자는 Acknowledgment.acknowledge()를 해야 한다. 그 이후에는 BATCH 와 동일한 시맨틱이 적용된다.
  * 이 옵션을 사용할 경우에는 AcknowledgingMessageListener 또는 BatchAcknowledgingMessageListener 를 리스너로 사용해야 한다.
* MANUAL_IMMEDIATE
  * 리스너가 Acknowledgment.acknowledge() 메서드를 호출하면 즉시 오프셋을 커밋한다.
  * 이 옵션을 사용할 경우에는 AcknowledgingMessageListener 또는 BatchAcknowledgingMessageListener 를 리스너로 사용해야 한다.

</details>

### 컨슈머 그룹이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

컨슈머는 컨슈머 그룹 안에 속한 것이 일반적인 구조로, 하나의 컨슈머 그룹안에 여러 개의 컨슈머가 구성될 수 있다.  
하나의 토픽에 여러개의 파티션이 있을 때, 컨슈머들은 파티션과 매칭되어 메세지를 가져오게된다.  
따라서, 컨슈머의 개수가 아무리 많다고 한들 파티션의 개수가 적다면, 대기중인 컨슈머가 많아질 수 있다.  

![image](https://github.com/user-attachments/assets/822be331-cd5a-49f4-8440-e5d7bcc79456)

</details>

### Partition 의 개수는 어느정도가 적당할까?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

여러 개의 컨슈머는 동일한 컨슈머 그룹 아이디로 컨슈머 그룹으로 묶을 수 있다.  
파티션과 컨슈머 그룹 내의 컨슈머는 N:1 관계로 파티션은 컨슈머 그룹 내에서 단 하나의 컨슈머에만 매핑될 수 있지만, 컨슈머는 다양한 파티션을 매핑할 수 있다.  
따라서 최대한 성능을 뽑아내려면 토픽의 파티션 개수와 컨슈머의 개수를 일치시키는 것이 좋다.  
최초 토픽 생성 시, 파티션 개수를 지정할 수 있다. 파티션 개수는 늘릴 수는 있지만 줄일 수는 없으므로 다음을 고려해야 한다.  
추가로 파티션을 줄일 수 없는 이유는 여러 이유가 있겠지만, 대표적인 이유로 다수 브로커에 분배되어 있는 세그먼트를 다시 재배열하는 것에 상당한 리소스가 들어가기 때문이다.  
* 프로듀서와 컨슈머 데이터 처리량
  * 프로듀서 전송량 < 컨슈머 데이터 처리량 * 파티션 개수
  * ex) 프로듀서가 초당 1000개의 레코드를 보고, 컨슈머가 초당 100개의 레코드를 처리할 수 있다면 해당 토픽의 파티션은 최소 10개 이상이어야 한다.
* 메시지 키 사용 여부(데이터 처리 순서)
  * 메시지 키를 사용하면 메시지 키 해싱값으로 파티션을 결정하므로 메시지 키가 같다면 해당 레코드는 항상 같은 파티션으로 전송된다.
  * 처리 순서가 보장되어야 한다면 최대한 파티션 변화가 발생하지 않는 방식으로 운영해야 하고 파티션 개수가 변해야만 한다면 커스텀 파티셔너를 개발해야 한다. 따라서 애초부터 파티션 개수를 넉넉하게 잡는 것이 권장된다.
* 브로커 영향도
  * 파티션이 늘어나는 만큼 브로커에서 접근하는 파일 개수가 많아진다. 데이터 양이 많아져서 파티션 개수를 늘려야하는 상황이라면 브로커 당 파티션 개수를 확인하고 파티션 개수가 너무 많다면 카프카 브로커 개수를 늘려야한다.
  * 즉, 세그먼트를 저장할 때 브로커의 파일 시스템을 이용하여서 파티션이 늘어날수록 브로커가 접근하는 파일 개수도 많아진다.
  * 열려있는 파일이 너무 많으면 운영체제에서 새롭게 파일에 접근하는 작업이 거부될 수 있어서 브로커 당 파티션 개수를 확인해보고 파티션의 개수를 결정하는게 좋다.
* 리더 선출 및 ISR 복제
  * 파티션의 개수가 너무 많으면 리더를 선출하는 과정에서 굉장히 오랜 시간이 걸릴 수 있기에 너무 많은 것도 좋지 않다.
  * 마찬가지로 복제시에도 파티션이 너무 많으면 복제를 해야할 양이 많아지므로 응답 시간이 느려질 수 있다.

</details>

### ISR 이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

ISR 은 리더 파티션과 팔로워 파티션이 모두 싱크된 상태를 의미한다.  
팔로워 파티션은 리더 파티션의 데이터를 복제하는데 시간이 걸리기 때문에 싱크가 안된 시점이 존재한다.  
따라서 일정 주기로 복제되었는지를 확인하고 복제되지 않는다면 해당 팔로워 파티션은 ISR 그룹에서 제외된다.  
ISR 그룹에 묶인 파티션은 모두 동일한 데이터가 존재함을 보장하기 때문에 리더 파티션 선출 자격을 갖는다.  

리더와 팔로워는 ISR(InSyncReplica)라는 논리적 그룹으로 묶여있다.  
ISR 그룹 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다.  
리더는 팔로워들이 뒤쳐지지 않고 리플리케이션을 잘하고 있는지를 감시한다. -> 리플리케이션을 잘 못한다면, 리더가 ISR 그룹에서 방출시킨다.  
* 예시로, 팔로워의 네트워크 오류, 팔로워 브로커 장애, 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는 경우 등에 방출  

ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게 된다.  
마지막 커밋 오프셋 위치는 **하이워터마크(High Watermark)** 라고 부른다.  
그리고, 이렇게 커밋된 메세지만 컨슈머가 읽을 수 있다.  
모든 브로커는 재시작될 때, 커밋된 메세지를 유지하기 위해 **로컬 디스크의 replication-offset-checkpoint** 라는 파일에 마지막 커밋 오프셋 위치를 저장한다.  

</details>

### Replication 에서 리더 파티션과 팔로워 파티션은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

카프카는 내부적으로 모두 동일한 리플리케이션(Replication)들을 리더와 팔로워로 구분하고, 각자의 역할을 분담한다.  

리더  
* 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 리더를 통해서만 가능하다.
* 프로듀서가 특정 토픽으로 메세지를 전송하면, 파티션의 리더만 읽고, 쓰기가 가능하므로 파티션의 리더로 메세지를 보내고, 컨슈머에서도 리더로부터 메세지를 가져온다.

팔로워  
* 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.
* 컨슈머가 토픽의 메세지를 꺼내 가는 것과 비슷한 동작으로 지속적으로 파티션의 리더가 새로운 메세지를 받았는지 확인하고 새로운 메세지가 있다면 복제한다.

</details>

### Replication 동작 순서는?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

팔로워들은 주기적으로 리더에게 메세지 **가져오기 요청(Fetch Request)** 을 보낸다.  
가져오기 요청의 응답에는 **Offset, Leader Epoch, 하이워터마크** 가 포함되어 있다.  
리더는 팔로워들이 메세지를 가져갔을 때 현재 가져간 메세지가 복제에 성공했는지 안했는지에 대해서는 모른다.  
* Rabbit MQ 의 트랜잭션 모드에서는 모든 미러(카프카로 치면 팔로워)가 메세지를 받았는지 ACK 를 리더에게 리턴하므로, 리더는 미러들이 메세지를 받았는지 알 수 있다.
* 하지만, 카프카는 리더와 팔로워 간 ACK 를 주고받는 통신은 없다. -> 성능이 더 빠르다.

팔로워는 특정 메세지 복제를 완료했으면 다음 메세지를 또 요청한다. 이 때 리더는 가져오기 요청이 온 메세지 이전까지의 메세지는 복제가 다 완료되었다고 인지하고, 이전 메세지까지 커밋하고 하이워터마크를 1 증가시킨다.  
결론은, 팔로워들이 Pull 방식으로 리더의 데이터를 가져가는 방식을 채택하면서 리더의 부하를 줄여 성능을 높였다.  

</details>

### 프로듀서에서 Acks 란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

> https://learn.conduktor.io/kafka/kafka-topic-replication/#Kafka-producers-acks-setting-3

프로듀서가 카프카 토픽의 리더에게 메시지를 보낸 후 요청 완료 전 ACK 를 받아야 하는 수를 의미한다. 옵션의 수가 작을 수록 성능이 좋아지지만 메시지 손실 가능성이 높아진다.  
acks 는 0, 1, all 세 가지 옵션으로 설정할 수 있다.  
acks = 0 : 프로듀서는 브로커가 메시지를 수락할 때까지 기다리지 않고 메시지를 전송한 순간 메시지를 성공적으로 작성된 것으로 간주한다.  
acks = 1 : 프로듀서는 메시지가 리더에게만 승인되면 성공적으로 작성된 메시지로 간주한다.  
acks = all : 프로듀서는 모든 동기화 복제본(ISR)이 메시지를 수락하면 메시지를 성공적으로 작성된 것으로 간주한다.  

일반적인 메시징 보상 수준을 다음과 같이 나타내기도 한다.  

At Most Once(최대 1회 전달)  
* ack = 0
* 메시지가 유실될 수 있지만, 중복 메시지가 발행되진 않는다.

At Least Once(최소 1회 전달)  
* ack=all or ack=1
* 결과는 적어도 한 번은 전달되며 중복 가능성이 있다.

Exactly once(정확히 1회 전달)  
* acks=all, idempotence=true
* 중복없이 정확히 한번 전달된다.

</details>

### min.insync.replicas 의 역할은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

> https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html#min-insync-replicas

프로듀서가 메세지를 보낼 때, 쓰기를 성공하기 위한 최소한의 복제본 수를 의미힌다.  
즉, ACK 응답을 보내기 위한 리더가 확인해야할 최소 리플리케이션의 수를 지정하는 브로커 관련 옵션이다.  
근데, acks=all 로 설정했다고해도, min.insync.replicas 가 1 이라면, 리더만 메세지를 수신해도 리플리케이션의 조건이 충족되기 때문에 ACK 응답을 보내게 된다.  
또한, acks=all 일 때 min.insync.replicas 를 replication factor 수와 똑같이 한다면, 브로커 하나가 장애 났을 때 가용 가능한 브로커 수는 min.insync.replicas 개수 보다 하나가 적게 될 텐데 이 때, 메세지를 수신하면 replication factor 수가 적기 때문에 무조건 에러가 발생한다.  
따라서, acks=all 이면, min.insync.replicas 는 2로 설정하는 것이 안정적이고, 성능도 좋다.  

</details>

### 리더 에포크란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

리더에포크(Leader Epoch)는 카프카의 파티션들이 복구 동작을 할 때, 메세지의 일관성을 유지하기 위한 용도로 사용된다.  
리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자  
리더에포크 정보는 리플리케이션 프로토콜에 의해 전파(정확히는 팔로워가 가져오기 요청을 하면 응답으로 리더 에포크를 전달한다.)  
리더 에포크는 리더가 장애가 발생했을 때만 변화가 있다.  
리더 에포크는 고유의 번호이다.  

![image](https://images.ctfassets.net/gt6dp23g0g38/7kr6K36N4VF4D5F3gpY71h/10329b5e4b700afdb1aa28a46432fd44/Kafka_Internals_033.png)
> The leader will respond to the fetch request with the records starting at the specified offset. 
> The fetch response will also include the offset for each record and the current leader epoch. 
> The followers will then append those records to their own local logs.
> 리더는 지정된 오프셋부터 시작하는 레코드를 포함하여 fetch 요청에 응답합니다. 
> fetch 응답에는 각 레코드의 오프셋과 현재 리더 에포크도 포함됩니다. 
> 이후 팔로워들은 해당 레코드들을 자신의 로컬 로그에 추가합니다.

</details>

### acks=all 이고 min.insync.replicas=1 일 때 복제와 상관없이 리더만 잘 받았다면 내부적으로 커밋이 되는가?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

min.insync.replicas 가 1 이라면 ISR 내세어 리더만 데이터 수신에 성공하면 프로듀서에게 ACK 응답을 보낸다.  
ISR 내에서 모든 복제가 완료되면, 리더가 내부적으로 커밋을 한다.  
이 마지막 커밋을 하이워터마크라고 하는데, 하이워터마크는 모든 팔로워가 복제에 성공해야만 올라간다.  

</details>

### 하이워터마크는 올라가지 않았지만, min.insync.replicas 값을 낮춰서 프로듀서에게 ACK 응답을 빠르게 보냈을 때 얻어지는 이점은 무엇인가 ?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

데이터 손실을 최소화할 수 있다. -> 프로듀서에게 ACK 가 반환되면 프로듀서는 메세지가 적어도 리더에 저장되었음을 신뢰할 수 있다.  
ISR 재구성 시에도 팔로워 중 하나가 장애가 나도 리더는 ISR 을 재구성하면서 자신만으로 데이터를 안전하게 복구할 수 있다.  
min.insync.replicas 를 1로 권장하지 않는 이유는 리더가 장애났을 때 데이터 유실이 될 수 있기 때문에, 2로 해서 팔로워 하나 정도는 안전하게 복제 하는 것이 좋다.  
팔로워 복제 여부와 상관없이 min.insync.replicas 조건만 만족하면 프로듀서가 빠르게 응답을 받을 수 있어서 성능적으로도 좋다.  

</details>

### 팔로워가 많을 수록 메세지를 소비하는 속도가 많이 느려지는가 ?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

하이워터마크는 팔로워의 복제 완료를 기준으로 한다.  
따라서 팔로워가 많으면 복제하는데 오래걸려서 속도가 느려질 가능성이 있다.  

</details>

### 하이워터마크가 올라가는 시점은 모든 팔로워가 복제가 완료된 시점인데, 모든 팔로워가 복제를 완료했다는걸 리더가 어떻게 아는가?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

> https://levelup.gitconnected.com/high-water-mark-hwm-in-kafka-offsets-5593025576ac

팔로워는 리더에게 주기적으로 가져오기 요청을 보낸다.  
리더는 가져오기 요청을 수신하면 팔로워가 요청한 Offset 을 보고 이전까지는 복제를 완료했다고 인지하고 ISR 내 상태를 업데이트한다. (하이워터마크증가)  
결국 모든 팔로워가 가져오기 요청을 보냄으로써 특정 오프셋에 도달했는지 확인하고, 모든 ISR 멤버가 동일한 오프셋에 도달해야 하이워터마크가 올라간다.  
또한, 리더는 팔로워가 일정 시간 내에 복제를 못하면 해당 팔로워는 ISR 에서 제거된다.  
replica.lag.max.messages 옵션으로 팔로워가 리더에게 너무 많이 뒤쳐지는 것을 방지할 수 있음  
fetch.max.wait.ms 옵션으로 팔로워가 리더에서 데이터를 가져오는 주기를 조정하여 복제 속도를 최적화할 수 있음  

</details>

### 하이워터마크가 올라가면 팔로워들은 올라간 하이워터마크를 어떻게 알까?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

ISR 내 모든 팔로워가 동일한 오프셋에 올라가는게 확인되면 하이워터마크를 1 올리는 작업을 시작한다.  
하이워터마크가 1 올라가면 그 즉시, 리더가 팔로워들한테 전파하는 것이 아니다.  
팔로워는 주기적으로 Fetch 요청을 보내면서 데이터를 가져온다.  
리더는 Fetch 요청에 대한 응답에 현재의 하이워터마크 값을 포함하여 팔로워에게 전달한다.  
이 과정에서 하이워터마크 정보는 팔로워가 요청을 보낸 시점의 상태에 기반하여 전달하게된다.  
즉, 리더가 하이워터마크를 갱신한 시점과 팔로워가 이를 알게되는 시점은 다를 수 있다.  

</details>

### 팔로워가 Offset 2 를 가져오기 요청했다고 하면, 리더는 Offset 1 까지 복제가 완료되었다고 인지하고 하이워터마크를 1로 설정하는 건지?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

아니다, 모든 ISR 의 복제가 완료되어야한다.  
즉, 특정 팔로워가 Offset 2 를 가져오기 요청을 해도 다른 팔로워가 아직 요청을 안했으면 하이워터마크를 올릴 수 없다.  
모든 팔로워가 Offset 3 가져오기 요청을 해야만 모든 팔로워가 Offset 2 를 복제했다는걸 인지하고 그때 하이워터마크를 올린다.  

</details>

### 리더 에포크의 역할은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

리더 에포크는 데이터 무결성 보장을 위한 용도로 사용된다.  
리더 에포크는 특정 파티션의 리더가 변경될 때마다 증가하는 숫자 값이다.  
새로운 리더가 선출되면, 리더 에포크가 증가하며 새로운 리더의 유효성을 나타내는 고유한 식별자 역할을 한다.  
모든 클라이언트와 팔로워는 리더 에포크를 기준으로 "현재의 유효한 리더"를 판단할 수 있다.  
장애가 발생한 기존 리더가 다시 살아날 수도 있지만, 기존 리더가 이미 유효하지 않다는 것을 보장하기 위해 리더 에포크를 사용한다.  
새로운 리더의 에포크가 더 크기 때문에, 모든 팔로워와 클라이언트는 새 리더만을 신뢰하고 기존 리더로부터의 혼란을 방지합니다.  

예시  
* 리더 A의 에포크: 5
* 팔로워 B가 새로운 리더로 선출됨 → 에포크: 6
* 리더 A가 복구되더라도 에포크 5의 리더로 동작할 수 없음.

클라이언트가 데이터 쓰기(Produce) 또는 읽기(Fetch) 요청을 보낼 때, 리더 에포크를 확인한다.  
클라이언트는 요청을 보낸 리더가 여전히 유효한 리더인지 에포크를 기준으로 판단한다.  
만약 클라이언트가 오래된 리더(낮은 에포크)에 요청을 보낸 경우, 새로운 리더로 재시도하도록 유도된다.  
클라이언트가 에포크 5의 리더 A에게 요청을 보냈지만, 이 리더가 이미 장애 상태라면, 에포크 6의 새로운 리더 B로 요청을 재전송한다.  

</details>

### 리더가 장애가 나서 복구하는 과정에 리더의 Offset 과 팔로워의 Offset 간 차이가 있고, 하이워터마크 차이도 있으면 리더에 있던 Offset 은 사라지는가?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

리더는 오프셋 5번 까지 저장했고, 현재 하이워터마크가 3이라고 가정했을 때 리더가 장애가 났다.  
그럼 리더의 오프셋 4번, 오프셋 5번은 복제를 하지 못한 상태로 팔로워가 새로운 리더로 선출될 것 이다.  
그러면 오프셋 4번, 오프셋 5번은 사라지게 된다. 왜냐면 카프카는 데이터 무결성 보장"을 우선하며, 커밋되지 않은 데이터를 손실하는 것을 허용하기 때문이다.  
다만, min.insync.replicas 를 2로 설정한 상황에서 만약 리더가 장애가 났다면, 프로듀서는 ACK 응답을 받지 못했음으로 재시도를 할 수 있다.  
따라서, 이때는 오프셋 4번, 오프셋 5번을 프로듀서가 다시 보냄으로써 데이터 손실을 방지할 수 있다.  

</details>

### 프로듀서에 메세지를 전송할 때 배치 전송은 왜 하는 것이고 해당 옵션 종류는?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

프로듀서에서는 처리량을 높이기 위해 배치 전송을 권장한다.  
buffer.memory : 카프카로 메세지들을 전송하기 위해 담아두는 프로듀서의 버퍼 메모리 옵션 (기본값 32MB)  
batch.size : 배치 전송을 위해 메세지들을 묶는 단위를 설정하는 배치 크기 옵션 (기본값 16KB)  
linger.ms : 배치 전송을 위해 메세지들을 묶는 시간을 설정하는 옵션 (기본값 0)  
지연 없는 전송이 목표라면 batch.size 와 linger.ms 를 작게 설정해야 한다.  
처리량을 높이려면 batch.size 와 linger.ms 를 늘려야 한다.  
높은 처리량을 목표로 할 경우에는 buffer.memory 가 커야하고 반드시 buffer.memory 는 batch.size 보다 커야한다.  
예를 들어, batch.size 가 16KB 이고 파티션이 3개일 때, 16KB * 3 = 48KB 이므로 buffer.memory 는 48KB 보다 커야한다.  
왜냐하면 프로듀서는 전송에 실패하면 재시도를 수행하는데, 이때 재시도를 위해 메모리를 사용하기 때문이다.  

</details>

### 프로듀서 메세지 전송 전략 중 라운드 로빈 방식은 무엇이고 문제점은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

메세지의 키 값을 지정하지 않으면 예전에는 기본적으로 라운드 로빈 방식으로 메세지를 전송했다.  
근데 이 방식은 문제가 있었다.  
보통 파티셔너를 거친 메세지들은 배치 처리를 위해 프로듀서의 버퍼 메모리 영역에서 잠시 대기한 후 카프카로 전송된다.  
만약 배치 전송을 위한 최소 레코드(메세지) 수가 3으로 설정되어 있고 파티션의 수가 3개라고 가정했을 때, 메세지가 총 5개 전송되었고 라운드 로빈 방식이라면 계속해서 카프카로 전송되지 못한 채 프로듀서 내에서 대기할 것 이다.  
왜냐하면 1번 파티션에 2개 2번 파티션에 2개 3번 파티션에 1개가 보관되어있으니 어떤 파티션도 전송할 수 없다.  
이러한 문제를 해결하기 위해 스티키 파티셔닝이 나왔다.  

</details>

### 프로듀서 메세지 전송 전략 중 스티키 파티셔닝 방식은 무엇인가?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

스티키 파티셔닝 전략이란 하나의 파티션에 메세지 수를 먼저 채워서 카프카로 빠르게 배치 전송하는 전략이다.  
Confluent 에서는 라운드 로빈 방식에 비해 30% 이상 지연시간이 감소하고 프로듀서의 CPU 사용률도 줄어드는 효과를 얻었다고한다.  
> https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/  

</details>

### 프로듀서에서 중복 없는 전송은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

프로듀서로부터 브로커가 중복된 메세지를 받으면 브로커는 ACK 응답만 프로듀서에게 보내는 기법  
프로듀서는 메세지 전송 시 PID(producerId) 와 메세지 번호(sequence)를 헤더에 포함해서 전달하고 브로커는 PID 와 메세지 번호를 비교해서 중복되었다면 ACK 만 응답한다.  

예시  
* 프로듀서가 브로커의 특정 토픽으로 메세지 A 를 전송한다. 이때 PID 와 메세지 번호를 헤더에 포함해서 전달한다.
* 브로커는 메세지 A 를 기록하고 PID 와 메세지 번호를 메모리에 기록한다. 그리고 ACK 를 프로듀서에게 응답한다.
* 프로듀서는 다음 메세지인 메세지 B 를 전송한다. 이때 PID 와 메세지 번호를 헤더에 포함해서 전달한다.
* 브로커는 메세지 B 를 기록하고 PID 와 메세지 번호를 메모리에 기록한다. 그리고 ACK 를 프로듀서에게 응답하려했지만 네트워크 오류로 전달되지 않았다.
* ACK 를 받지 못한 프로듀서는 브로커가 메세지 B 를 받지 못했다고 판단하여 재전송한다.
* 브로커는 이미 메세지 B 를 기록했기 때문에 PID 와 메세지 번호를 비교해서 이미 있다면 ACK 만 응답한다.

옵션  
* enable.idempotence : 중복 없는 전송을 위한 옵션 (Spring Kafka 3.x 부터 기본값 true)
  * max.in.flight.requests.per.connection = 1이상 5이하 값 : 한번에 보낼 수 있는 배치 개수
  * idempotence 사용하려면 5 이하여야하는 조건이 있다.
* max.in.flight.requests.per.connection : ACK 를 받지 않은 상태에서 하나의 커넥션에서 보낼 수 있는 최대 요청 수 (기본값 5)
* acks : all 로 설정해야한다.
* retries : 재시도 횟수, ACK 를 받지 못한 경우 재시도를 해야하므로 0보다 큰 값으로 설정
* min.insync.replicas : ISR 그룹 내 복제 최소 개수

</details>

### 프로듀서의 메세지 전송 방식 3가지는?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

적어도 한 번 전송  
최대 한 번 전송  
정확히 한 번 전송  

</details>

### 프로듀서 메세지 전송 방식 중 적어도 한 번 전송은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

카프카의 가장 기본적인 동작 방식이다.

네트워크의 회선 장애나 기타 장애 상황에 따라 일부 메세지 중복이 발생할 수 있지만, 최소한 하나의 메세지는 반드시 보장하는 방법  
예시  
* 프로듀서가 브로커의 특정 토픽으로 메세지 A 를 전송한다.
* 브로커는 메세지 A 를 기록하고 잘 받았다는 ACK 를 프로듀서에게 응답한다.
* 브로커의 ACK 를 받은 프로듀서는 다음 메세지인 메세지 B 를 전송한다.
* 브로커는 메세지 B 를 기록하고 잘 받았다는 ACK 를 프로듀서에게 응답하려 했지만 네트워크 오류로 전달되지 않았다.
* ACK 를 받지 못한 프로듀서는 메세지 B 를 재전송한다.

</details>

### 프로듀서 메세지 전송 방식 중 최대 한 번 전송은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

일부 메세지가 손실되더라도 높은 처리량을 필요로 하는 대량의 로그 수집같은 환경에서 사용 하곤 한다.  
사실 최대 한 번 전송에서는 ACK 를 응답하는 과정은 필요 없다. (예시에만 등장)  

예시  
* 프로듀서가 브로커의 특정 토픽으로 메세지 A 를 전송한다.
* 브로커는 메세지 A 를 기록하고 잘 받았다는 ACK 를 프로듀서에게 응답한다.
* 브로커의 ACK 를 받은 프로듀서는 다음 메세지인 메세지 B 를 전송한다.
* 브로커는 메세지 B 를 기록하고 잘 받았다는 ACK 를 프로듀서에게 응답하려 했지만 네트워크 오류로 전달되지 않았다.
* 프로듀서는 브로커가 메세지 B 를 받았다고 가정하고 메세지 C 를 전송한다.

</details>

### 프로듀서 메세지 전송 방식 중 정확히 한 번 전송은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

중복 없는 전송 방식이 정확히 한 번 전송하는 것은 아니다.  
카프카 트랜잭션을 활용하면 정확히 한 번 전송할 수 있다.  
프로듀서가 보내는 메세지들은 원자적으로 처리되어 전송에 성공하거나 실패하게 된다.  
트랜잭션을 사용하면 전체 consume-가공-produce 프로세스를 하나의 원자 트랜잭션으로 취급할 수 있으며, 모든 단계가 성공할 때만 커밋된다.  
어느 지점에서든 실패가 발생하면 전체 트랜잭션이 중단된다. 이렇게 하면 메세지가 중복되거나 데이터가 더 심각하게 손상되는 것을 방지할 수 있다.  
**transactional.id**는 프로듀서 수준에서 설정되며, 트랜잭션 프로듀서를 애플리케이션이 재시작된 후에도 식별할 수 있도록 한다.  
**transaction coordinator**는 트랜잭션 메타데이터를 관리하고 전체 트랜잭션 프로세스를 감독하는 브로커 프로세스이다.  
**transaction coordinator**는 전송된 메세지를 관리하며, 커밋 또는 중단 등을 표시한다.  
**transaction coordinator**는 트랜잭션 로그를 내부 토픽인 __transaction_state 에 저장한다.  
__transaction_state 는 내부 토픽이고 이 역시 토픽이므로 파티션 수와 리플리케이션 팩터 수가 존재하며 브로커의 설정을 통해 변경가능하다.  
프로듀서가 직접 __transaction_state 에 기록하는게 아니라 transaction coordinator 에게 알리고, 모든 정보는 transaction coordinator 가 직접 기록한다.  
transaction coordinator 는 transactional.id 의 해시를 가져와서 이를 사용하여 __transaction_state 토픽의 파티션을 결정합니다. 이곳에 트랜잭션 로그를 저장한다.  

멱등성 프로듀서의 한계는 동일한 세션 내에서만 정확히 한 번의 전달을 보장한다.  
여기서 세션은 PID(Producer ID)의 생명주기를 의미한다.  
만약 멱등성 프로듀서로 작동하는 프로듀서 애플리케이션에 문제가 발생해 종료되고 다시 시작하면 새로운 PID가 생성된다.  

멱등성 프로듀서는 장애가 발생하지 않는 상황에서만 데이터를 정확히 한 번 적재하는 것을 보장한다는 점을 명심해야 한다.  
동일한 데이터를 전송하더라도, PID가 바뀌면 브로커는 다른 프로듀서 애플리케이션이 다른 데이터를 보냈다고 판단해서 중복해서 메세지가 보내질 수 있다.  

</details>

### 카프카 트랜잭션 시작부터 종료까지 과정은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

트랜잭션이 시작되면 프로듀서는 파티션 정보를 트랜잭션 코디네이터에게 전달하고, 코디네이터는 트랜잭션 로그에 기록한다.  
이 때, TID 와 파티션 정보가 로그에 기록되며 트랜잭션 상태를 Ongoing 으로 표시한다. 이 때, 기본값인 1분동안 상태 업데이트가 없으면 트랜잭션은 실패로 처리된다.  

메세지 전송을 완료하면 프로듀서는 commit 혹은 abort 메서드 중 하나를 무조건 호출해야한다.  
해당 메세드를 통해 트랜잭션의 종료를 코디네이터에게 알린다.  
트랜잭션 코디네이터는 두 단계의 커밋과정을 거친다.  
첫 번째로 트랜잭션 로그에 해당 트랜잭션에 대한 PrepareCommit 또는 PrepareAbort 를 기록한다.  
두 번째로 트랜잭션 로그에 기록된 토픽의 파티션에 트랜잭션 커밋 또는 롤백 표시를 기록한다.  
예를 들어, 특정 파티션에 메세지를 전송했고, 해당 메세지의 오프셋이 1이라고 가정했을 때 코디네이터는 해당 파티션에 트랜잭션 커밋 표시를 기록하고 이 때 파티션의 마지막 오프셋은 2로 증가한다.  
트랜잭션이 성공하지 않으면 오프셋은 증가할 일이 없으며 이렇게 되면 컨슈머에 절대 반환되지 않는다.  

트랜잭션이 종료되면 트랜잭션 코디네이터는 완료됨이라고 트랜잭션 로그에 기록한다.  
프로듀서에게 해당 트랜잭션이 완료됨을 알린 다음 해당 트랜잭션에 대한 처리는 모두 마무리된다.  

트랜잭션을 이용한 컨슈머는 read_committed 레벨로 설정해야한다.  
왜냐하면 커밋된 메세지만 읽어야하는데, read_uncommitted 로 설정되면 커밋되지 않은 메세지도 소비하기 때문이다.  

</details>

### 컨슈머 그룹이 오프셋을 관리하는 방식은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

컨슈머 그룹은 오프셋 정보를 카프카에서 가장 안전한 저장소인 토픽에 저장한다. 이 토픽의 이름은 __consumer_offsets 이다.  
__consumer_offsets 토픽에는 각 컨슈머 그룹별로 오프셋 위치 정보가 기록된다.  
컨슈머들은 지정된 토픽의 메세지를 읽은 뒤, 읽어온 위치의 오프셋 정보를 __consumer_offsets 토픽에 기록한다.  
이때 컨슈머 그룹은 컨슈머 그룹, 토픽, 파티션 등의 내용을 통합해 기록한다.  
__consumer_offsets 토픽에 기록된 정보를 이용해 컨슈머 그룹은 자신의 그룹이 속해 있는 컨슈머의 변경이 발생하면 해당 컨슈머가 어디까지 읽었는지를 추적할 수 있다.  
여기저 저장되는 오프셋 값은 마지막까지 읽은 위치는 아니고 컨슈머가 다음으로 읽어야 할 위치다.  

</details>

### 컨슈머 그룹 리밸런싱이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

리밸런싱은 컨슈머 그룹 내에서 파티션 할당을 다시 조정하는 과정을 의미한다.   

eager  
* 리밸런싱 시 기존 컨슈머들의 모든 파티션 할당을 취소하고 재 할당 전까지 메시지를 소비하지 않는다.

cooperative  
* 리밸런싱 시 모든 파티션 할당을 취소하지 않고 대상이 되는 컨슈머들에 대해서 파티션에 따라 점진적으로 컨슈머를 할당한다. 
* 예를 들어, consumer1(partiton 1,3), consumer2(partition2) 상태에서 consumer3가 추가된다면 partition3만 consumer3로 매핑이 이동된다.

리밸런싱이 발생하는 경우  
* 컨슈머가 추가되는 상황
* 컨슈머가 종료되는 상황(종료되고 코디네이터가 이를 인지하지 못해도 heartbeat 간격 수신을 받지 못하면 종료된 것으로 판단)

</details>

### 컨슈머 그룹 코디네이터란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

컨슈머들은 하나의 컨슈머 그룹의 구성원으로 속한다.  
컨슈머 그룹 내의 각 컨슈머들은 서로 자신의 정보를 공유하며 하나의 공동체로 동작한다.  
컨슈머 그룹 내의 컨슈머들은 언제든지 자신이 속한 컨슈머 그룹을 떠날 수 있으며 새로운 컨슈머가 합류할 수도 있다.  
컨슈머 그룹에서 각 컨슈머들에게 작업을 균등하게 분배하는 동작을 컨슈머 리밸런싱이라고 부른다.  
그룹 코디네이터는 컨슈머 그룹의 안정성과 리밸런싱 과정을 주도한다.  
그룹 코디네티어는 각 컨슈머 그룹별로 존재하며, 그룹 코디네이터는 카프카 클러스터 내 브로커 중 하나에 위치한다.  
컨슈머 그룹이 브로커에 최초 연결 요청을 보내면 브로커 중 하나에 그룹 코디네이터가 생성되고 이 그룹 코디네이터는 컨슈머 그룹의 변경과 구독하는 토픽 파티션 변견 등에 대한 감지를 시작한다.  
그리고 토픽의 파티션과 그룹의 멤버 변경이 일어나면 변경된 내용들을 컨슈머들에게 알려준다.  

</details>

### 컨슈머 그룹에 등록되는 과정은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

1. 컨슈머는 bootstrap.brokers 리스트에 있는 브로커에게 컨슈머 클라이언트와 초기 커넥션을 연결하기 위한 요청을 보낸다.
2. 요청을 받은 브로커는 그룹 코디네이터를 생성하고 컨슈머에게 응답을 보낸다.
3. 그룹 코디네이터는 group.initial.rebalance.delay.ms 시간 동안 컨슈머의 요청을 기다린다.
4. 컨슈머는 컨슈머 등록 요청을 그룹 코디네이터에게 보낸다. 이때 가장 먼저 요청한 컨슈머가 그룹의 리더가 된다.
5. 컨슈머 등록 요청을 받은 그룹 코디네이터는 해당 컨슈머 그룹이 구독하는 토픽 파티션 리스트 등 리더 컨슈머의 요청에 응답을 보낸다.
6. 리더 컨슈머는 정해진 컨슈머 파티션 할당 전략에 따라 그룹 내 컨슈머들에게 파티션을 할당한 뒤 그룹 코디네이터에게 전달한다.
7. 그룹 코디네이터는 해당 정보를 캐시하고 각 그룹 내 컨슈머들에게 성공을 알린다.
8. 각 컨슈머들은 각자 지정된 토픽 파티션으로부터 메세지들을 가져온다.

</details>

### 리밸런싱 과정은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

![image](https://github.com/user-attachments/assets/4659a9da-07cc-4f1e-8c58-401181c61425)

eager 일 경우  
1. 컨슈머가 추가/종료된 경우 기존 컨슈머들은 그룹에 다시 조인하기 위해 코디네이터에 poll 요청을 보낸다.
2. 코디네이터는 모든 컨슈머로부터 poll 요청 을 받으면 컨슈머 그룹 리더를 선정한다.
3. 리더는 각 컨슈머에게 파티션 할당을 결정하고 결정된 사항을 코디네이터에게 전달한다.
4. 팔로워들은 할당된 파티션 정보를 얻기 위해 코디네이터에게 다시 요청한다.

cooperative 일 경우  
1. 컨슈머가 추가/종료된 경우 기존 컨슈머들은 그룹에 다시 조인하기 위해 코디네이터에 poll 요청을 보낸다.
2. 코디네이터는 연관된 일부 컨슈머로부터 poll 요청 을 받으면 컨슈머 그룹 리더를 선정한다.
3. 리더는 각 컨슈머에게 파티션 할당을 결정하고 결정된 사항을 코디네이터에게 전달한다.
4. 팔로워들은 할당된 파티션 정보를 얻기 위해 코디네이터에게 다시 요청한다.

</details>

### 리밸런싱 과정에서 그룹 내 특정 컨슈머가 poll 메소드를 호출하지 않은 경우는 어떻게 동작하는가?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

그룹 내 모든 컨슈머들이 조인 요청을 보내야만 코디네이터가 리더를 선출한다고 보면 된다.  
그렇다면 특정 컨슈머가 poll 메소드를 호출하지 못하고 있어서 조인 요청을 보내지 못한다면 리더 선출도 일어나지 않게 될까?  
사실 컨슈머의 조인 요청에는 rebalanceTimeout을 포함하고 있다.  
컨슈머들은 파티션 리밸런싱이 시작된 이후에 rebalanceTimeout 시간내에 조인 요청을 보내야 한다.  
만약 컨슈머가 rebalanceTimeout 이내에 조인 요청을 보내지 못하는 경우 그룹에서 제외된다.  
RebalanceTimeout은 카프카 0.10.1 버전 이후에 추가되었다.  
그리고 KafkaConsumer 1.1.0 기준으로 rebalanceTimeout은 max.poll.interval.ms 값으로 세팅된다.  
따라서 파티션 리밸런싱이 일어난 이후에 max.poll.interval.ms 시간내에 조인 요청을 보내지 못한다면, 해당 컨슈머는 그룹에서 제외된다.  

</details>

### 컨슈머 배포 시 고려해야할 사항은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

![image](https://github.com/user-attachments/assets/423512be-5667-403b-af53-14c86b94e914)
eager나 cooperative 모드 둘다 리밸런싱 되는 경우, 전체/일부 컨슈머가 조인 요청을 보내고 파티션을 할당받기 까지 소비가 중단된다.  
이때 먼저 poll을 한 컨슈머가 있고 레코드를 처리하고 있는 컨슈머가 처리가 조금 오래걸려서 처리 이후에 poll을 한 경우, 미리 poll을 요청한 컨슈머는 그만큼 기다리게 된다.  
레코드 데이터 처리 시간이 짧다면 문제가 없지만 긴 경우에는 컨슈머 lag이 쌓이는 문제가 발생한다.  
poll 메소드의 호출 간격은 (레코드 하나를 처리하는데 걸리는 시간) X (poll 메소드를 통해 가져온 레코드의 수)  
따라서 poll 메소드의 호출 간격을 줄이기 위해서는 레코드 하나를 처리하는데 걸리는 시간을 줄이거나, 한 번의 poll 메소드를 통해 가져오는 레코드의 수를 줄여야 한다.  
하지만 레코드를 처리하는 과정에서 외부 시스템에 의존하거나, DB를 조회하는 등 여러 가지 작업이 일어나는 경우 레코드 처리 시간을 줄이기는 어렵다.  
반면에 poll 메소드를 통해 가져오는 레코드의 수를 줄이기는 매우 쉽다.  
컨슈머의 max.poll.records 속성을 변경하기만 하면 된다.  
위에서 설명했듯이 max.poll.records 속성은 poll 메소드 호출을 통해 가져올 수 있는 레코드의 최대 수를 조정한다.  
즉, 컨슈머의 데이터 처리 속도가 느리다면 max.poll.records 옵션을 작게 설정하는 방법이 있다.  
max.poll.records 를 작게 설정하면 성능에 문제가 생기는건 아닌지 우려한다.  
하지만 실제로 max.poll.records 를 작게 설정하더라고 성능에 큰 영향을 주지는 않는다.  
그 이유는 컨슈머가 레코드를 가져올 때 Fetcher 라는 클래스를 사용하기 때문이다.  

![image](https://github.com/user-attachments/assets/ba75f193-b3ef-48dc-9495-07a4704592b2)

컨슈머는 poll 메소드가 호출되면, Fetcher의 fetchedRecords 메소드를 호출한다.  
fetchedRecords 메소드는 최대 max.poll.records 만큼의 레코드를 리턴한다.  
만약에 Fetcher가 레코드를 가지고 있지 않다면, fetchedRecords 메소드는 빈 Map을 반환한다.  
그리고 빈 Map이 반환된 경우에만 컨슈머는 Fetcher#sendFetches 메소드를 호출한다.  
sendFetches 메소드에서는 Fetcher가 브로커에게 요청을 보내 레코드를 가져온다.  
하나의 요청으로 최대 fetch.max.bytes 크기만큼 가져올 수 있고, 파티션당 최대 max.partition.fetch.bytes 크기만큼 가져올 수 있다.  
그리고 요청은 현재 컨슘하고 있는 파티션의 리더에게 모두 보낸다.  
예를 들어, 현재 컨슈머가 컨슘하고 있는 파티션이 0, 1이고 0번 파티션의 리더가 브로커 1, 1번 파티션의 리더가 브로커 2라면, 컨슈머는 브로커 1, 2에게 각각 요청을 보낸다. 즉 2개의 요청을 보내게 된다.  
Fetcher#sendFetches 메소드를 호출 후, 컨슈머는 또다시 fetchedRecords 메소드를 호출한다.  
그러면 Fetcher는 요청을 통해 가져온 레코드 중에서 최대 max.poll.records 만큼의 레코드를 반환한다.  
컨슈머의 poll 메소드가 또다시 호출이 된 경우, 컨슈머는 Fetcher#fetchedRecords 메소드를 호출한다.  
이전 요청을 통해 가져온 레코드가 아직 남아있다면, Fetcher는 브로커에게 요청을 보내지 않고 가지고 있는 레코드 중에서 max.poll.records 만큼의 레코드를 바로 반환한다.  

결과적으로 Fetcher는 가지고 있는 레코드가 없는 경우에만 브로커에게 요청을 보낸다.  
따라서 max.poll.records를 작게 설정해도 성능에 큰 영향을 주진 않는다.  
오히려 fetch.max.bytes 설정과 max.partition.fetch.bytes 설정이 성능에 큰 영향을 준다.  

결과적으로 하나의 레코드를 처리하는 시간이 너무 길어지면 이 때는 스레드의 분리를 고려할 수 밖에 없다.  
즉, 컨슈머 스레드와 레코드를 처리하는 스레드를 분리하는 것이다.  
Consumer 에서 메세지를 받고 레코드를 별도의 스레드 풀에서 처리하는 모델을 사용하면 리밸런싱 성능이 향상될 수 있다.  

</details>

### 컨슈머에서 파티션 할당 전략 종류는?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

레인지 파티션 할당 전략 (Range)  
라운드 로빈 파티션 할당 전략 (RoundRobin)  
스티키 파티션 할당 전략 (Sticky)  
협력적 스티키 파티션 할당 전략 (Cooperative Sticky)  

</details>

### 레인지 파티션 할당 전략

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

파티션 할당 전략 중 기본값  
먼저 구독하는 토픽에 대한 파티션을 순서대로 나열 후 컨슈머를 순서대로 정렬하고 각 컨슈머가 몇 개의 파티션을 할당해야 하는지 전체 파티션 수를 컨슈머 수로 나눈다.  
컨슈머 수와 파티션 수가 일치하면 균등하게 나눠지고 아니라면 앞쪽의 컨슈머들은 추가 할당을 받게 된다.  
예를 들어 토픽1에 파티션이 3개 있고 컨슈머 그룹 내 컨슈머가 2개라면 3/2 = 1 이므로 컨슈머당 최소 한개를 가져야한다. 균등하게 나눠지지 않았으므로 첫 번쨰 컨슈머가 2개를 할당받게 된다.  
동일한 메세지 키를 사용하고 하나의 컨슈머 그룹이 동일한 파티션 수를 가진 2개 이상의 토픽을 컨슘할 때 유용하다. 즉, 동일한 컨슈머가 동일한 키를 계속 소비하므로 일관성을 보장할 수 있다.  

</details>

### 라운드 로빈 파티션 할당 전략이란?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

먼저 소비해야 하는 모든 파티션과 그룹 내 모든 컨슈머를 나열한 후 라운드 로빈으로 하나씩 파티션과 컨슈머를 할당하는 방식  
따라서, 레인지 파티션보다는 균등하게 할당된다.  

</details>

### 스티키 파티션 할당 전략

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

레인지 파티션과 라운드 로빈 파티션 둘 다 리밸런싱 동작으로 파티션이 재할당되면 동일한 컨슈머에 파티션이 매핑되리라고는 보장할 수 없다.  
스티키 파티션 전략은 재할당 작업이 발생하더라도 기존에 매핑됐던 파티션과 컨슈머를 최대한 유지하려고 하는 전략이다.  
스티키 파티션 전략은 가능한 한 균형 잡힌 파티션 할당, 재할당이 발생할 때 되도록 기존에 할당된 파티션 정보를 보장하는 것을 원칙으로 한다. 여기서 전자가 우선순위가 더 높다. 즉, 무조건 기존 파티션과 컨슈머를 유지하지는 않는다.  
라운드 로빈 방식은 파티션이 재할당되면 모든 파티션과, 컨슈머를 순서대로 배치후 처음부터 하나씩 할당한다.  
스티키 파티션 전략은 특정 컨슈머가 제외되었을 떄 기존에 할당되어있던 파티션은 그대로 두고 제외된 컨슈머에 할당되었었던 파티션들만 재할당을 하게된다.  
스티키 파티션 전략은 다음과 같은 규칙이 있다.  
* 컨슈머들의 최대 할당된 파티션의 수의 차이는 1  
* 기존에 존재하는 파티션 할당은 최대한 유지함  
* 재할당 동작 시 유효하지 않은 모든 파티션 할당은 제거함  
* 할당되지 않은 파티션들은 균형을 맞추는 방법으로 컨슈머들에 할당  

</details>

### 협력적 스티키 파티션 할당 전략은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

결과만 보면 스티키 파티션 할당 전략과 동일하다.  
협력적 스티키 파티션 할당 전략은 컨슈머 내부 리밸런싱 동작이 한층 고도화되었다.  
컨슈머 리밸런싱이 일어나면 내부적으로 EAGER 라는 리밸런스 프로토콜을 사용한다.  
EAGER 프로토콜은 컨슈머 리밸런싱 동작 시 컨슈머에 할당된 모든 파티션을 항상 취소한다.  
리밸런싱 중 모든 파티션을 취소하면 컨슈머의 다운타임이 시작된다. 이 때 다운타임 동안 LAG 이 급격하게 증가할 수 있다.  
카프카 2.3 버전 이후부터는 협렵적 스티키는 내부 리밸런싱 프로토콜을 COOPERATIVE 을 사용했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머의 상태를 유지할 수 있다.   
즉, 동작 중인 컨슈머들에게 영향을 주지 않는 상태에서 몇 차례에 걸쳐 리밸런싱이 이뤄진다.  

EAGER 프로토콜의 취소 이유  
* 컨슈머들의 파티션 소유권 변경 때문 -> A 컨슈머가 갖고 있는 0번 파티션의 소유권을 B 컨슈머에게 할당해야할 때 하나의 컨슈머 그룹 내에서는 둘 이상의 컨슈머가 동일한 파티션을 소유할 수 없으므로
* 그룹 내에서 여러 파티션들에 대해 소유권 변경 작업이 동시에 이뤄져야하는데 이 로직을 단순하게 구현하기 위해서

예를 들어 파티션이 3개 있고 그룹 내 컨슈머가 2개 있는 상황에서 컨슈머 하나가 새롭게 합류한다고 가정하면 다음과 같다.  
* 컨슈머 그룹에 consumer3 이 합류하면서 리밸런싱이 트리거된다.
* 그룹 내 컨슈머들은 그룹 합류 요청과 자신들이 컨슘하는 토픽의 파티션 정보를 그룹 코디네이터로 전송한다.
* 그룹 코디네이터는 해당 정보를 조합해 컨슈머 그룹의 리더에게 전송한다.
* 그룹의 리더는 현재 컨슈머들이 소유한 파티션 정보를 활용해 제외해야 할 파티션 정보를 담은 새로운 파티션 할당 정보를 컨슈머 그룹 멤버들에게 전달한다. (리밸런싱 첫 번째 진행)
* 제외된 파티션 할당을 위해 컨슈머들은 다시 합류 요청을 합니다.
* 컨슈머 그룹의 리더는 제외된 파티션을 적절한 컨슈머에게 할당한다. (리밸런싱 두 번째 진행)
* 리밸런싱을 2번에 결쳐서 진행했고, 기존 컨슈머들은 다운타임 없이 계속 동작할 수 있다.

EAGER 보다 COOPERATIVE 성능 비교 결과 : https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/

</details>

### 파티션 하나에 메세지가 몰린다면?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

카프카 브로커의 특정 파티션 키에 메시지가 지나치게 몰리는 상황이 발생하면 랙이 쌓이게 된다. 다음과 같은 해결책을 고려해볼 수 있다.  

파티션 키 수정 : 파티션 키가 고루 분선되지 않기 때문이므로 다른 파티션 키를 고려해본다.  
파티셔너를 직접 구현하는 것도 방법이다. 대규모에서는 고른 분산을 위해 Consistent Hashing(해시 링)을 사용하기도 한다.  
파티션 수 늘리기 : 많은 파티션에 메시지가 분산될 수 있도록 한다.  
컨슈머 수 늘리기 : 파티션 수 만큼 컨슈머를 늘린다면 파티션과 컨슈머가 1:1 매핑된다.  
메시지 처리 병렬화 : Consumer 쪽에서 쓰레드 풀을 사용하여 병렬로 처리한다.  

</details>

### Producer 의 처리량을 높일 수 있는 옵션은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------

compression.type : 압축 옵션  
* 압축을 사용하면 producer가 메시지 압축에 사용된 cpu 시간 때문에 대기 시간을 추가하지만 잠재적 디스크 쓰기를 줄여 처리량을 높일 수 있다.

batch.size(단일 배치 사이즈 크기) 와 linger.ms(배치로 메시지를 보내기 위한 최대 대기 시간)  
* batch.size가 꽉 차거나 linger.ms 시간에 도달하면 메시지를 전송하기 때문에 두 옵션의 조절로 처리량이 높은 단일 생성 요청에 더 많은 메시지를 배치할 수 있다.

buffer.memory : 버퍼에 사용할 총 메모리 양(Record accumulator의 전체 메모리 사이즈)  
* 버퍼 크기는 배치 크기만큼 커야하며 버퍼링, 압축 및 진행 중 요청을 수용할 수 있을 정도의 크기여야 한다.

max.request.size : 브로커에게 보낼 수 있는 전체 합산 최대 메시지 크기  
* batch.size를 늘린다면 그에 맞춰 더 늘려야 한다.

send.buffer.bytes : 카프카 브로커와 통신할 때 사용하는 TCP 소켓의 버퍼 크기  
* 이 버퍼는 프로듀서가 메시지를 네트워크를 통해 브로커로 보내기 전에 데이터를 일시적으로 저장하는 공간
* 네트워크 환경이 불안정하거나 지연이 발생할 때, 이 버퍼 크기를 늘리면 프로듀서가 더 많은 데이터를 일시적으로 저장하고 네트워크 전송 대기 시간을 줄일 수 있어 전송 성능이 향상

</details>

### Consumer 의 처리량을 높일 수 있는 옵션은?

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>

-----------------------


fetch.min.bytes : 컨슈머가 브로커에서 데이터를 읽어들이기 위해 기다리는 최소 데이터 크기  
* 브로커는 지정된 만큼 새로운 메시지가 쌓일때까지 전송하지 않는다.
* 이 값을 높이면 컨슈머가 더 많은 데이터를 한 번에 가져와 처리할 수 있으므로, 네트워크 호출 횟수를 줄이고 효율성을 높일 수 있다. 다만, 너무 높게 설정하면 지연이 발생

max.partition.fetch.bytes : 컨슈머가 파티션별로 가져올 수 있는 최대 데이터 크기를 설정  
* 여러 파티션에서 데이터를 동시에 가져올 때, 이 값을 높이면 한 번에 더 많은 데이터를 가져올 수 있어 처리량이 증가할 수 있다. fetch.max.bytes에 제약을 받는다.

fetch.max.bytes : 컨슈머가 한 번에 가져올 수 있는 최대 데이터 크기  
* 이 값을 늘리면 더 많은 데이터를 한 번에 가져올 수 있어 처리량이 증가할 수 있으나 시스템 메모리와 네트워크 대역폭을 고려해 적절하게 설정

fetch.wait.max.ms : fetch.min.bytes 조건이 충족될 때까지 브로커가 대기하는 최대 시간  
* 컨슈머가 데이터를 가져오기 위해 너무 오래 기다리지 않도록 하여 성능을 향상

receive.buffer.bytes : 컨슈머가 데이터를 읽을 때 사용하는 TCP 수신 버퍼의 크기  
* 이 값을 늘리면 네트워크에서 수신하는 데이터 처리 성능이 향상될 수 있으나 시스템 메모리 사용량에 영향을 줄 수 있으므로 적절하게 설정

max.poll.records : fetcher의 버퍼로부터 컨슈머가 한 번에 poll로 가져올 수 있는 레코드의 최대 개수  
* 이 값을 늘리면 컨슈머가 더 많은 레코드를 한 번에 가져와 처리할 수 있으므로, 처리량이 증가할 수 있으나 처리량이 증가하면서 메모리 사용량도 증가할 수 있으므로 주의
* 하지만 이 옵션보다는 fetch.max.bytes와 max.partition.fetch.bytes 설정이 더 큰 영향을 준다.

max.poll.interval.ms : poll 호출 간의 최대 대기 시간을 설정  

max.poll.records 옵션 줄이기  
* 해당 옵션은 fetcher의 버퍼로부터 컨슈머가 한 번에 poll로 가져올 수 있는 레코드의 최대 수이기 때문에 줄여도 큰 영향이 없다.(fetcher의 버퍼로부터 가져오는 것이기 때문)
* 이 옵션을 줄여서 빠르게 poll한 데이터만 처리한 후 조인 요청을 하여 리밸런싱한다.

적절한 조정이 필요하겠지만 보통 대기시간(latency)를 낮추려면 위 옵션들의 값을 줄여야하고 처리량을 높이려면 값을 높여야한다.  

일반적으로 kafka는 producer 단에서는 전송만 하면 되기 때문에 문제가 없지만 컨슈머쪽에서는 메시지를 소비해서 실질적인 로직을 처리하기 때문에 소비 속도를 판단하여 조치를 취하지 않는다면 lag이 쌓이게 된다.  
이를 위해서 컨슈머 쪽에서는 다음과 같은 방안을 취할 수 있다.  

멀티 쓰레드 컨슈머  
* 파티션과 컨슈머는 N:1 관계이지만 컨슈머의 개수를 파티션 개수와 일치시킨다면 1:1 매핑이 되어 처리량을 늘릴 수 있다.
* 실제로 다수의 프로세스를 띄워서 다수의 컨슈머를 만들어도 되지만 spring kafka 에서는 concurrency 옵션으로 멀티 스레드 컨슈머를 만들 수 있다.
* OOM을 주의해야 한다.

멀티 워커 쓰레드  
* 배치로 레코드를 소비한 뒤, ExecutorService를 사용하여 별도의 스레드 풀로 처리를 위임하여 병렬 처리하는 형태
* 병렬 처리로 인해 처리 순서가 섞일 수 있기 때문에 처리 순서가 중요하다면 메시지 키별로 동일한 스레드를 할당받도록 하는 별도의 추가 로직을 구성해야 한다.
* executorService를 사용할 경우, 메인 스레드는 기다리지 않기 때문에 별도의 대기 후 offset을 수동 커밋해줘야 한다.

</details>