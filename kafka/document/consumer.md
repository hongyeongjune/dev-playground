## Consumer

### 컨슈머 오프셋 관리
* 컨슈머 그룹은 오프셋 정보를 카프카에서 가장 안전한 저장소인 토픽에 저장한다. 이 토픽의 이름은 __consumer_offsets 이다.
* __consumer_offsets 토픽에는 각 컨슈머 그룹별로 오프셋 위치 정보가 기록된다.
* 컨슈머들은 지정된 토픽의 메세지를 읽은 뒤, 읽어온 위치의 오프셋 정보를 __consumer_offsets 토픽에 기록한다.
* 이때 컨슈머 그룹은 컨슈머 그룹, 토픽, 파티션 등의 내용을 통합해 기록한다.
* __consumer_offsets 토픽에 기록된 정보를 이용해 컨슈머 그룹은 자신의 그룹이 속해 있는 컨슈머의 변경이 발생하면 해당 컨슈머가 어디까지 읽었는지를 추적할 수 있다.
* 여기저 저장되는 오프셋 값은 마지막까지 읽은 위치는 아니고 컨슈머가 다음으로 읽어야 할 위치다.

### 리밸런싱
* Eager
  * 리밸런싱 시 기존 컨슈머들의 모든 파티션 할당을 취소하고 재 할당 전까지 메시지를 소비하지 않는다.
* cooperative
  * 리밸런싱 시 모든 파티션 할당을 취소하지 않고 대상이 되는 컨슈머들에 대해서 파티션에 따라 점진적으로 컨슈머를 할당한다.
  * 예를 들어, consumer1(partiton 1,3), consumer2(partition2) 상태에서 consumer3가 추가된다면 partition3만 consumer3로 매핑이 이동된다.
* 리밸런싱이 발생하는 경우 
  * 컨슈머가 추가되는 상황 
  * 컨슈머가 제외되는 상황 
    * 종료되고 코디네이터가 이를 인지하지 못해도 heartbeat 간격 수신을 받지 못하면 종료된 것으로 판단

![images](https://github.com/user-attachments/assets/96c05962-c441-4594-9338-7589f16e9784)

1. 컨슈머가 추가/종료된 경우 기존 컨슈머들은 그룹에 다시 조인하기 위해 코디네이터에 조인 요청을 보낸다. 
2. 코디네이터는 모든 컨슈머로부터 조인 요청 을 받으면 컨슈머 그룹 리더를 선정한다.
    * 그룹 내 특정 컨슈머가 rebalanceTimeout 시간 내에 poll을 호출하지 않은 경우 컨슈머 그룹에서 제외된다. 
3. 리더는 각 컨슈머에게 파티션 할당을 결정하고 결정된 사항을 코디네이터에게 전달한다. 
4. 팔로워들은 할당된 파티션 정보를 얻기 위해 코디네이터에게 다시 요청한다.

* 위 과정은 poll 메서드 안에서 진행된다. 위 과정은 eager이지만 cooperative는 2번 과정에서 연관된 일부 컨슈머만 조인 요청을 보낸다는 차이가 있다.
* poll 메서드는 컨슈머가 브로커로부터 메시지를 가져오는 메서드

* 그룹 내 특정 컨슈머가 poll 메소드를 호출하지 않은 경우
  * 그룹 내 모든 컨슈머들이 조인 요청을 보내야만 코디네이터가 리더를 선출한다고 된다.
  * 그렇다면 특정 컨슈머가 poll 메소드를 호출하지 못하고 있어서 조인 요청을 보내지 못한다면 리더 선출도 일어나지 않게 될까?
  * 사실 컨슈머의 조인 요청에는 rebalanceTimeout을 포함하고 있다.
  * 컨슈머들은 파티션 리밸런싱이 시작된 이후에 rebalanceTimeout 시간내에 조인 요청을 보내야 한다.
  * 만약 컨슈머가 rebalanceTimeout 이내에 조인 요청을 보내지 못하는 경우 그룹에서 제외된다.
  * RebalanceTimeout은 카프카 0.10.1 버전 이후에 추가되었다.
  * 그리고 KafkaConsumer 1.1.0 기준으로 rebalanceTimeout은 max.poll.interval.ms 값으로 세팅된다.
  * 따라서 파티션 리밸런싱이 일어난 이후에 max.poll.interval.ms 시간내에 조인 요청을 보내지 못한다면, 해당 컨슈머는 그룹에서 제외된다.

![images](https://github.com/user-attachments/assets/e2c5d4ec-790f-4621-814c-cf7f43c26924)

* eager나 cooperative 모드 둘다 리밸런싱 되는 경우, 전체/일부 컨슈머가 조인 요청을 보내고 파티션을 할당받기 까지 소비가 중단된다. 
* 이때 먼저 poll을 한 컨슈머가 있고 레코드를 처리하고 있는 컨슈머가 처리가 조금 오래걸려서 처리 이후에 poll을 한 경우, 미리 poll을 요청한 컨슈머는 그만큼 기다리게 된다.
* 레코드 데이터 처리 시간이 짧다면 문제가 없지만 긴 경우에는 컨슈머 lag이 쌓이는 문제가 발생한다.
* poll 메소드의 호출 간격은 (레코드 하나를 처리하는데 걸리는 시간) X (poll 메소드를 통해 가져온 레코드의 수)
* 따라서 poll 메소드의 호출 간격을 줄이기 위해서는 레코드 하나를 처리하는데 걸리는 시간을 줄이거나, 한 번의 poll 메소드를 통해 가져오는 레코드의 수를 줄여야 한다.
* 하지만 레코드를 처리하는 과정에서 외부 시스템에 의존하거나, DB를 조회하는 등 여러 가지 작업이 일어나는 경우 레코드 처리 시간을 줄이기는 어렵다.
* 반면에 poll 메소드를 통해 가져오는 레코드의 수를 줄이기는 매우 쉽다. 
* 컨슈머의 max.poll.records 속성을 변경하기만 하면 된다. 
* 위에서 설명했듯이 max.poll.records 속성은 poll 메소드 호출을 통해 가져올 수 있는 레코드의 최대 수를 조정한다.
* 즉, 컨슈머의 데이터 처리 속도가 느리다면 max.poll.records 옵션을 작게 설정하는 방법이 있다. 
* max.poll.records 를 작게 설정하면 성능에 문제가 생기는건 아닌지 우려한다.
* 하지만 실제로 max.poll.records 를 작게 설정하더라고 성능에 큰 영향을 주지는 않는다.
* 그 이유는 컨슈머가 레코드를 가져올 때 Fetcher 라는 클래스를 사용하기 때문이다.

![Image](https://github.com/user-attachments/assets/acd2cc9e-2e38-4cf5-9d41-5feb19611d25)

* 컨슈머는 poll 메소드가 호출되면, Fetcher의 fetchedRecords 메소드를 호출한다. 
* fetchedRecords 메소드는 최대 max.poll.records 만큼의 레코드를 리턴한다.
* 만약에 Fetcher가 레코드를 가지고 있지 않다면, fetchedRecords 메소드는 빈 Map을 반환한다.
* 그리고 빈 Map이 반환된 경우에만 컨슈머는 Fetcher#sendFetches 메소드를 호출한다.
* sendFetches 메소드에서는 Fetcher가 브로커에게 요청을 보내 레코드를 가져온다.
* 하나의 요청으로 최대 fetch.max.bytes 크기만큼 가져올 수 있고, 파티션당 최대 max.partition.fetch.bytes 크기만큼 가져올 수 있다.
* 그리고 요청은 현재 컨슘하고 있는 파티션의 리더에게 모두 보낸다. 
* 예를 들어, 현재 컨슈머가 컨슘하고 있는 파티션이 0, 1이고 0번 파티션의 리더가 브로커 1, 1번 파티션의 리더가 브로커 2라면, 컨슈머는 브로커 1, 2에게 각각 요청을 보낸다. 즉 2개의 요청을 보내게 된다.
* Fetcher#sendFetches 메소드를 호출 후, 컨슈머는 또다시 fetchedRecords 메소드를 호출한다. 
* 그러면 Fetcher는 요청을 통해 가져온 레코드 중에서 최대 max.poll.records 만큼의 레코드를 반환한다.
* 컨슈머의 poll 메소드가 또다시 호출이 된 경우, 컨슈머는 Fetcher#fetchedRecords 메소드를 호출한다. 
* 이전 요청을 통해 가져온 레코드가 아직 남아있다면, Fetcher는 브로커에게 요청을 보내지 않고 가지고 있는 레코드 중에서 max.poll.records 만큼의 레코드를 바로 반환한다.

* 결과적으로 Fetcher는 가지고 있는 레코드가 없는 경우에만 브로커에게 요청을 보낸다. 
* 따라서 max.poll.records를 작게 설정해도 성능에 큰 영향을 주진 않는다. 
* 오히려 fetch.max.bytes 설정과 max.partition.fetch.bytes 설정이 성능에 큰 영향을 준다.

* 결과적으로 하나의 레코드를 처리하는 시간이 너무 길어지면 이 때는 스레드의 분리를 고려할 수 밖에 없다.
* 즉, 컨슈머 스레드와 레코드를 처리하는 스레드를 분리하는 것이다.
* Consumer 에서 메세지를 받고 레코드를 별도의 스레드 풀에서 처리하는 모델을 사용하면 리밸런싱 성능이 향상될 수 있다. 

### 리밸런싱 위험
* 리밸런싱은 컨슈머의 소유권을 재조정하기 때문에 리밸런싱이 발생한 컨슈머 그룹 내의 모든 컨슈머의 읽기 작업은 중단됨
* 컨슈머의 일시적인 서비스 중단이 발생할 수 있음
* 리밸런싱은 컨슈머 쪽의 서비스 상태에 일시적인 영향을 줄 수 있으므로 컨슈머, 파티션의 추가가 필요하다면 충분한 고려 후에 추가해야함
* 추가적으로 하트비트 전송주기를 결정하는 hearbeat.interval.ms 설정과 session.timeout.ms 설정은 원치 않는 리밸런싱 발생에 영향을 미치므로 적절하게 설정할 필요가 있음. 대체로 1 : 3 비율로 설정하는 것을 권장

### 그룹 코디네이터
* 컨슈머들은 하나의 컨슈머 그룹의 구성원으로 속한다.
* 컨슈머 그룹 내의 각 컨슈머들은 서로 자신의 정보를 공유하며 하나의 공동체로 동작한다.
* 컨슈머 그룹 내의 컨슈머들은 언제든지 자신이 속한 컨슈머 그룹을 떠날 수 있으며 새로운 컨슈머가 합류할 수도 있다.
* 컨슈머 그룹에서 각 컨슈머들에게 작업을 균등하게 분배하는 동작을 컨슈머 리밸런싱이라고 부른다.
* 그룹 코디네이터는 컨슈머 그룹의 안정성과 리밸런싱 과정을 주도한다.
* 그룹 코디네티어는 각 컨슈머 그룹별로 존재하며, 그룹 코디네이터는 카프카 클러스터 내 브로커 중 하나에 위치한다.
* 컨슈머 그룹이 브로커에 최초 연결 요청을 보내면 브로커 중 하나에 그룹 코디네이터가 생성되고 이 그룹 코디네이터는 컨슈머 그룹의 변경과 구독하는 토픽 파티션 변견 등에 대한 감지를 시작한다.
* 그리고 토픽의 파티션과 그룹의 멤버 변경이 일어나면 변경된 내용들을 컨슈머들에게 알려준다.

### 컨슈머 그룹 등록 과정
1. 컨슈머는 bootstrap.brokers 리스트에 있는 브로커에게 컨슈머 클라이언트와 초기 커넥션을 연결하기 위한 요청을 보낸다.
2. 요청을 받은 브로커는 그룹 코디네이터를 생성하고 컨슈머에게 응답을 보낸다.
3. 그룹 코디네이터는 group.initial.rebalance.delay.ms 시간 동안 컨슈머의 요청을 기다린다.
4. 컨슈머는 컨슈머 등록 요청을 그룹 코디네이터에게 보낸다. 이때 가장 먼저 요청한 컨슈머가 그룹의 리더가 된다.
5. 컨슈머 등록 요청을 받은 그룹 코디네이터는 해당 컨슈머 그룹이 구독하는 토픽 파티션 리스트 등 리더 컨슈머의 요청에 응답을 보낸다.
6. 리더 컨슈머는 정해진 컨슈머 파티션 할당 전략에 따라 그룹 내 컨슈머들에게 파티션을 할당한 뒤 그룹 코디네이터에게 전달한다.
7. 그룹 코디네이터는 해당 정보를 캐시하고 각 그룹 내 컨슈머들에게 성공을 알린다.
8. 각 컨슈머들은 각자 지정된 토픽 파티션으로부터 메세지들을 가져온다.

### 스태틱 멤버십
* 카프카 2.3 버전 이전에서는 하트비트 주기, 세션 타임아웃 등 설정으로 컨슈머가 재시작할 때마다 전체 리밸런싱이 일어났다.
* 왜냐하면 컨슈머 그룹에서는 각 컨슈머를 식별하기 위해 엔티티 ID 를 부여하게 되는데, 그룹 내에서 임시로 사용되는 값이기 때문에 컨슈머가 재시작되면, 그룹 내 동일한 컨슈머임에도 새로운 컨슈머로 인식해 새로운 엔티티 ID 가 부여되고 리밸런싱이 일어나게된다.
* 카프카 2.3 버전 이후에서는 스태틱 멤버실을 도입하여 컨슈머가 재시작 등으로 그룹을 나갔다 합류하더라도 리밸런싱이 일어나지 않는다.
* 즉, 컨슈머마다 인식할 수 있는 고유의 ID 를 부여하여 다시 합류하더라도 그룹 코디네이터가 기존 구성원임을 인식할 수 있게 한다.
* 또한, 스태틱 멤버십이 적용된 컨슈머는 그룹에서 떠날 때 그룹 코디네이터에게 알리지 않으므로 불필요한 리밸런싱도 발생하지 않게된다.
* ```group.instance.id``` 만 설정하면 스태틱 멤버십이 적용된다.
* 스태틱 멤버십이 적용되면 ```session.timeout.ms```를 기본값보다는 큰 값으로 조정해야한다. 컨슈머를 재시작한 후 ```session.timeout.ms``` 값에 지정된 시간동안 그룹 코디네이터가 하트비트를 받지 못하면 리밸런싱이 일어나게 될 수 있기 때문이다.

### 레인지 파티션 할당 전략
* 파티션 할당 전략 중 기본값
* 먼저 구독하는 토픽에 대한 파티션을 순서대로 나열 후 컨슈머를 순서대로 정렬하고 각 컨슈머가 몇 개의 파티션을 할당해야 하는지 전체 파티션 수를 컨슈머 수로 나눈다.
* 컨슈머 수와 파티션 수가 일치하면 균등하게 나눠지고 아니라면 앞쪽의 컨슈머들은 추가 할당을 받게 된다.
* 예를 들어 토픽1에 파티션이 3개 있고 컨슈머 그룹 내 컨슈머가 2개라면 3/2 = 1 이므로 컨슈머당 최소 한개를 가져야한다. 균등하게 나눠지지 않았으므로 첫 번쨰 컨슈머가 2개를 할당받게 된다.
* 동일한 메세지 키를 사용하고 하나의 컨슈머 그룹이 동일한 파티션 수를 가진 2개 이상의 토픽을 컨슘할 때 유용하다. 즉, 동일한 컨슈머가 동일한 키를 계속 소비하므로 일관성을 보장할 수 있다.

### 라운드 로빈 파티션 할당 전략
* 먼저 컨슘해야 하는 모든 파티션과 그룹 내 모든 컨슈머를 나열한 후 라운드 로빈으로 하나씩 파티션과 컨슈머를 할당하는 방식
* 따라서, 레인지 파티션보다는 균등하게 할당된다.

### 스티키 파티션 할당 전략
* 레인지 파티션과 라운드 로빈 파티션 둘 다 리밸런싱 동작으로 파티션이 재할당되면 동일한 컨슈머에 파티션이 매핑되리라고는 보장할 수 없다.
* 스티키 파티션 전략은 재할당 작업이 발생하더라도 기존에 매핑됐던 파티션과 컨슈머를 최대한 유지하려고 하는 전략이다.
* 스티키 파티션 전략은 가능한 한 균형 잡힌 파티션 할당, 재할당이 발생할 때 되도록 기존에 할당된 파티션 정보를 보장하는 것을 원칙으로 한다. 여기서 전자가 우선순위가 더 높다. 즉, 무조건 기존 파티션과 컨슈머를 유지하지는 않는다.
* 라운드 로빈 방식은 파티션이 재할당되면 모든 파티션과, 컨슈머를 순서대로 배치후 처음부터 하나씩 할당한다.
* 스티키 파티션 전략은 특정 컨슈머가 제외되었을 떄 기존에 할당되어있던 파티션은 그대로 두고 제외된 컨슈머에 할당되었었던 파티션들만 재할당을 하게된다.
* 스티키 파티션 전략은 다음과 같은 규칙이 있다.
  * 컨슈머들의 최대 할당된 파티션의 수의 차이는 1
  * 기존에 존재하는 파티션 할당은 최대한 유지함
  * 재할당 동작 시 유효하지 않은 모든 파티션 할당은 제거함
  * 할당되지 않은 파티션들은 균형을 맞추는 방법으로 컨슈머들에 할당

### 협력적 스티키 파티션 할당 전략
* 결과만 보면 스티키 파티션 할당 전략과 동일하다.
* 협력적 스티키 파티션 할당 전략은 컨슈머 내부 리밸런싱 동작이 한층 고도화되었다.
* 컨슈머 리밸런싱이 일어나면 내부적으로 EAGER 라는 리밸런스 프로토콜을 사용한다. EAGER 프로토콜은 컨슈머 리밸런싱 동작 시 컨슈머에 할당된 모든 파티션을 항상 취소한다.
* 취소하는 이유는 다음과 같다.
  * 컨슈머들의 파티션 소유권 변경 때문 -> A 컨슈머가 갖고 있는 0번 파티션의 소유권을 B 컨슈머에게 할당해야할 때 하나의 컨슈머 그룹 내에서는 둘 이상의 컨슈머가 동일한 파티션을 소유할 수 없으므로
  * 그룹 내에서 여러 파티션들에 대해 소유권 변경 작업이 동시에 이뤄져야하는데 이 로직을 단순하게 구현하기 위해서
* 리밸런싱 중 모든 파티션을 취소하면 컨슈머의 다운타임이 시작된다. 이 때 다운타임 동안 LAG 이 급격하게 증가할 수 있다.
* 카프카 2.3 버전 이후부터는 협렵적 스티키는 내부 리밸런싱 프로토콜을 COOPERATIVE 을 사용했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머의 상태를 유지할 수 있다. 즉, 동작 중인 컨슈머들에게 영향을 주지 않는 상태에서 몇 차례에 걸쳐 리밸런싱이 이뤄진다.
* 예를 들어 파티션이 3개 있고 그룹 내 컨슈머가 2개 있는 상황에서 컨슈머 하나가 새롭게 합류한다고 가정하면 다음과 같다.
  * 컨슈머 그룹에 consumer3 이 합류하면서 리밸런싱이 트리거된다.
  * 그룹 내 컨슈머들은 그룹 합류 요청과 자신들이 컨슘하는 토픽의 파티션 정보를 그룹 코디네이터로 전송한다.
  * 그룹 코디네이터는 해당 정보를 조합해 컨슈머 그룹의 리더에게 전송한다.
  * 그룹의 리더는 현재 컨슈머들이 소유한 파티션 정보를 활용해 제외해야 할 파티션 정보를 담은 새로운 파티션 할당 정보를 컨슈머 그룹 멤버들에게 전달한다. (리밸런싱 첫 번째 진행)
  * 제외된 파티션 할당을 위해 컨슈머들은 다시 합류 요청을 합니다.
  * 컨슈머 그룹의 리더는 제외된 파티션을 적절한 컨슈머에게 할당한다. (리밸런싱 두 번째 진행)
* 위 예시처럼 리밸런싱을 2번에 결쳐서 진행했고, 기존 컨슈머들은 다운타임 없이 계속 동작할 수 있다.
* EAGER 보다 COOPERATIVE 성능 비교 결과 : https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/

### 파티션 하나에 메세지가 몰린다면?
* 카프카 브로커의 특정 파티션 키에 메시지가 지나치게 몰리는 상황이 발생하면 랙이 쌓이게 된다. 다음과 같은 해결책을 고려해볼 수 있다. 

* 파티션 키 수정 : 파티션 키가 고루 분선되지 않기 때문이므로 다른 파티션 키를 고려해본다. 
* 파티셔너를 직접 구현하는 것도 방법이다. 대규모에서는 고른 분산을 위해 Consistent Hashing(해시 링)을 사용하기도 한다. 
* 파티션 수 늘리기 : 많은 파티션에 메시지가 분산될 수 있도록 한다. 
* 컨슈머 수 늘리기 : 파티션 수 만큼 컨슈머를 늘린다면 파티션과 컨슈머가 1:1 매핑된다. 
* 메시지 처리 병렬화 : Consumer 쪽에서 쓰레드 풀을 사용하여 병렬로 처리한다.

### 처리량이 높이기
* producer 성능 옵션
  * compression.type : 압축 옵션
    * 압축을 사용하면 producer가 메시지 압축에 사용된 cpu 시간 때문에 대기 시간을 추가하지만 잠재적 디스크 쓰기를 줄여 처리량을 높일 수 있다.
  * batch.size(단일 배치 사이즈 크기) 와 linger.ms(배치로 메시지를 보내기 위한 최대 대기 시간)
  * batch.size가 꽉 차거나 linger.ms 시간에 도달하면 메시지를 전송하기 때문에 두 옵션의 조절로 처리량이 높은 단일 생성 요청에 더 많은 메시지를 배치할 수 있다.
  * buffer.memory : 버퍼에 사용할 총 메모리 양(Record accumulator의 전체 메모리 사이즈)
    * 버퍼 크기는 배치 크기만큼 커야하며 버퍼링, 압축 및 진행 중 요청을 수용할 수 있을 정도의 크기여야 한다.
  * max.request.size : 브로커에게 보낼 수 있는 전체 합산 최대 메시지 크기 
    * batch.size를 늘린다면 그에 맞춰 더 늘려야 한다. 
  * send.buffer.bytes : 카프카 브로커와 통신할 때 사용하는 TCP 소켓의 버퍼 크기 
    * 이 버퍼는 프로듀서가 메시지를 네트워크를 통해 브로커로 보내기 전에 데이터를 일시적으로 저장하는 공간 
    * 네트워크 환경이 불안정하거나 지연이 발생할 때, 이 버퍼 크기를 늘리면 프로듀서가 더 많은 데이터를 일시적으로 저장하고 네트워크 전송 대기 시간을 줄일 수 있어 전송 성능이 향상 
* consumer 옵션 
  * fetch.min.bytes : 컨슈머가 브로커에서 데이터를 읽어들이기 위해 기다리는 최소 데이터 크기 
    * 브로커는 지정된 만큼 새로운 메시지가 쌓일때까지 전송하지 않는다. 
    * 이 값을 높이면 컨슈머가 더 많은 데이터를 한 번에 가져와 처리할 수 있으므로, 네트워크 호출 횟수를 줄이고 효율성을 높일 수 있다. 다만, 너무 높게 설정하면 지연이 발생 
  * max.partition.fetch.bytes : 컨슈머가 파티션별로 가져올 수 있는 최대 데이터 크기를 설정 
    * 여러 파티션에서 데이터를 동시에 가져올 때, 이 값을 높이면 한 번에 더 많은 데이터를 가져올 수 있어 처리량이 증가할 수 있다. fetch.max.bytes에 제약을 받는다. 
  * fetch.max.bytes : 컨슈머가 한 번에 가져올 수 있는 최대 데이터 크기 
    * 이 값을 늘리면 더 많은 데이터를 한 번에 가져올 수 있어 처리량이 증가할 수 있으나 시스템 메모리와 네트워크 대역폭을 고려해 적절하게 설정 
  * fetch.wait.max.ms : fetch.min.bytes 조건이 충족될 때까지 브로커가 대기하는 최대 시간 
    * 컨슈머가 데이터를 가져오기 위해 너무 오래 기다리지 않도록 하여 성능을 향상 
  * receive.buffer.bytes : 컨슈머가 데이터를 읽을 때 사용하는 TCP 수신 버퍼의 크기 
    * 이 값을 늘리면 네트워크에서 수신하는 데이터 처리 성능이 향상될 수 있으나 시스템 메모리 사용량에 영향을 줄 수 있으므로 적절하게 설정 
  * max.poll.records : fetcher의 버퍼로부터 컨슈머가 한 번에 poll로 가져올 수 있는 레코드의 최대 개수 
    * 이 값을 늘리면 컨슈머가 더 많은 레코드를 한 번에 가져와 처리할 수 있으므로, 처리량이 증가할 수 있으나 처리량이 증가하면서 메모리 사용량도 증가할 수 있으므로 주의 
    * 하지만 이 옵션보다는 fetch.max.bytes와 max.partition.fetch.bytes 설정이 더 큰 영향을 준다. 
  * max.poll.interval.ms : poll 호출 간의 최대 대기 시간을 설정 
* 리밸런싱의 영향 최소화 
  * max.poll.records 옵션 줄이기 
    * 해당 옵션은 fetcher의 버퍼로부터 컨슈머가 한 번에 poll로 가져올 수 있는 레코드의 최대 수이기 때문에 줄여도 큰 영향이 없다.(fetcher의 버퍼로부터 가져오는 것이기 때문)
    * 이 옵션을 줄여서 빠르게 poll한 데이터만 처리한 후 조인 요청을 하여 리밸런싱한다. 

* 적절한 조정이 필요하겠지만 보통 대기시간(latency)를 낮추려면 위 옵션들의 값을 줄여야하고 처리량을 높이려면 값을 높여야한다. 
* 일반적으로 kafka는 producer 단에서는 전송만 하면 되기 때문에 문제가 없지만 컨슈머쪽에서는 메시지를 소비해서 실질적인 로직을 처리하기 때문에 소비 속도를 판단하여 조치를 취하지 않는다면 lag이 쌓이게 된다. 
* 이를 위해서 컨슈머 쪽에서는 다음과 같은 방안을 취할 수 있다.

* 멀티 쓰레드 컨슈머 
  * 파티션과 컨슈머는 N:1 관계이지만 컨슈머의 개수를 파티션 개수와 일치시킨다면 1:1 매핑이 되어 처리량을 늘릴 수 있다. 
  * 실제로 다수의 프로세스를 띄워서 다수의 컨슈머를 만들어도 되지만 spring kafka 에서는 concurrency 옵션으로 멀티 스레드 컨슈머를 만들 수 있다. 
  * OOM을 주의해야 한다. 
* 멀티 워커 쓰레드 
  * 배치로 레코드를 소비한 뒤, ExecutorService를 사용하여 별도의 스레드 풀로 처리를 위임하여 병렬 처리하는 형태 
  * 병렬 처리로 인해 처리 순서가 섞일 수 있기 때문에 처리 순서가 중요하다면 메시지 키별로 동일한 스레드를 할당받도록 하는 별도의 추가 로직을 구성해야 한다. 
  * executorService를 사용할 경우, 메인 스레드는 기다리지 않기 때문에 별도의 대기 후 offset을 수동 커밋해줘야 한다.
